\documentclass[12pt,reqno]{amsart}
\input{preamble}
\graphicspath{{Figures/}}

%────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
%────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────

\begin{document}
\title{The Convex Sumset Problem} 

\author[Cushman]{Adam Cushman}
\address{College of Arts and Sciences\\
         Indiana University,
         Bloomington, IN 47405
         USA} 
\email{acushma@iu.edu}
\date{\today}

\begin{abstract}
The sum-product conjecture, posed in 1983 by Erd{\H{o}}s and Szemer{\'e}di \cite{erdos-szemeredi}, 
posits that any sufficiently large set must have a `relatively large' number
of distinct sums or products between its elements.
A newer conjecture extends this idea, positing that for convex sets there must be a `relatively large'
number of distinct sums between its elements. Both conjectures remain open and far from being solved.
This report provides an entirely self-contained overview of some known results, primarily focused on the
latter conjecture, and the methods used to achieve them.
\end{abstract}

\maketitle

%────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
%────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────

\section{Introduction and Motivation} 

For any sets \(A,B\) and binary operation \(\cdot \) which acts on elements of \(A\) and \(B\), we define
\[
    A \cdot  B = \left\{ a \cdot  b : a \in A~,~ b \in B \right\} 
.\]

Observe the following example which motivates the study of these problems.

Let \(A = \left\{ 1, 2, 3, \cdots, n \right\}\) and \(G = \left\{ 2, 2^{2}, 2^{3}, \cdots , 2^{n} \right\}\).

Notice that \(A\) is given by an arithmetic sequence and \(G\) by a geometric sequence.
We are going to calculate \(\left\lvert A+A \right\rvert , \left\lvert AA \right\rvert , \left\lvert G+G \right\rvert ,\) and \(\left\lvert GG \right\rvert \).

Observe that
\begin{align*}
    \left\lvert A + A \right\rvert  & = \left\lvert \left\{2,3, \cdots ,2n \right\}  \right\rvert \\
    & = 2n - 1 \\
    & = 2 \left\lvert A \right\rvert - 1,
\end{align*}
and by the same argument,
\begin{align*}
    \left\lvert GG \right\rvert  & = \left\lvert \left\{2^{2}, 2^{3}, \cdots , 2^{2n} \right\} \right\rvert  \\
    & =2 \left\lvert G \right\rvert -1.
\end{align*}
We have
\begin{align*}
    \left\lvert G + G \right\rvert & = \left\lvert \left\{ 2^{i} + 2^{j} : i,j \in \left\{ 1, \cdots , n \right\} \right\} \right\rvert
\end{align*}
For all \(i,j\), \(2^{i} + 2^{j}\) is a number written in base \(2\). By the uniqueness of binary representations
(see appendix), we have that \(2^{i} + 2^{j}\) is distinct for every choice of \(\left\{ i,j \right\} \). Therefore,
\begin{align*}
    \left\lvert G + G \right\rvert & \geq  \left\lvert \left\{ \left\{ i,j \right\} : i,j \in \left\{ 1, \cdots , n \right\}  \right\}  \right\rvert \\
    & = \binom{n}{2} + n \\
    & = \binom{\left\lvert G \right\rvert + 1}{2} 
\end{align*}

Finally, I prove in the appendix that there exists \(c \in \mathbb{R} ^{+}\) such that
\[
    \left\lvert AA \right\rvert \geq  \frac{c \left\lvert A \right\rvert ^{2}}{\log \left( \left\lvert A \right\rvert  \right) }
.\]

Observing the trivial bounds \( 2 \left\lvert S \right\rvert - 1 \leq \left\lvert S \cdot S \right\rvert \leq \binom{\left\lvert S \right\rvert }{2} \) for any set \(S\) and any
commutative operation \(\cdot \), it is clear that both \(AA\) and \(G +G\) are, as \(\left\lvert A \right\rvert, \left\lvert G \right\rvert \to \infty  \), almost as large as they can be,
and \(A + A, GG \) are as small as they can be.

This is the phenomenon which motivates this problem.
One questions is: ``does there exist a set for which neither the sum nor the product set is large?''
The sum-product conjecture states that such a set does not exist. 
Another question we study in this report is: ``what
determines if the sum set is large or the product set is large?'' A newer
conjecture which partially addresses this question states that the sum set is large
when the set itself is convex.

The rest of the report will proceed with preliminary definitions and ideas,
a precise statement of the main problems we'll focus on, proving a collection
of important results, and applying these results to the conjectures we are concerned with.

\section{Preliminaries}

As a useful shorthand, for any natural number \(n\), let
\[
    [n] = \left\{ 1, 2 , \cdots , n \right\} 
.\]

For a set \(S\), and function \(f : S \to \mathbb{R} \),
\[
    f(S) = \left\{ f(s) : s \in S\right\} 
.\]

The study of these problems requires the notion of orders of magnitude.
For any functions \(f,g: \mathbb{R}   \to \mathbb{R} \), write
\[
    f(x) \gg g(x) \text{ as } x \to \infty 
\]
if
\[
    \exists x_0,c \in \mathbb{R} ^{+}~~\text{s.t.}~~ x > x_0 \implies \left\lvert f(x) \right\rvert \geq c \left\lvert g(x) \right\rvert  
,\]
write
\[
    f(x) \ll g(x) \text{ as } x \to \infty 
\]
if
\[
     \exists x_0,c \in \mathbb{R} ^{+}~~\text{s.t.}~~ x > x_0 \implies \left\lvert f(x) \right\rvert \leq c \left\lvert g(x) \right\rvert 
,\]
and write
\[
    f(x) \asymp g(x) \text{ as } x \to \infty 
\]
if
\[
    f(x) \ll g(x)  ~ ~ \text{and} ~ ~ f(x) \gg g(x) \text{ as } x \to \infty 
.\]

We write \(\ll _{\epsilon},\gg _{\epsilon} \) if the constant depends on \(\epsilon\). For example,
\[
    f(x) \gg _{\epsilon} g(x) ^{ \epsilon}
\]
means
\[
   \forall \epsilon> 0~,~ f(x) \gg g(x) ^{\epsilon}
.\]
More precisely, there is some function \(c : \mathbb{R} ^{+ }\to \mathbb{R} ^{+}\) so
\[
    \forall \epsilon > 0, \exists x_0\in \mathbb{N} ~~\text{s.t.}~~ x > x_0 \implies \left\lvert f(x) \right\rvert \geq c (\epsilon)\left\lvert g(x) ^{\epsilon} \right\rvert
.\]

We write \(\lesssim , \gtrsim \) if, along with a constant factor, there is also a logarithmic factor.
That is
\[
    f(x) \lesssim  g(x)
\]
if there is some \(c \in \mathbb{R} \) such that
\[
    f(x) \ll \left( \log \left( x \right)  \right) ^{c} g(x)
.\]

Throughout this report, the asymptotic parameter (in this case \(x\)) will always tend to \(\infty \),
so it will no longer be mentioned.
Oftentimes the parameter will not even be in the expression. For example,
if for some set \(A\) we write
\[
    \left\lvert A + A \right\rvert \gg \left\lvert A \right\rvert 
,\]
it is taken to mean that \(A\) is defined implicitly by \(\left\lvert A \right\rvert \), and
\(\left\lvert A \right\rvert \) is the parameter which tends to \(\infty \).

For any sets \(A,B\) and any binary operation \(\cdot \) acting on elements of \(A\) and \(B\), define the representation function
\(r_{A \cdot B} : A \cdot B \to \mathbb{N} \) by
\[
    r_{A \cdot B} (x) = \left\lvert \left\{ (a,b)\in  A \times B : x = a \cdot b \right\}  \right\rvert
.\]

Throughout this report the shorthand
\[
    \delta_{A,B} (x) = r_{A - B} (x)~,~ \sigma _{A,B} (x) = r_{A + B} (x)~,~ \delta_{A} (x) = \delta_{A,A} (x) ~,~ \sigma_{A} (x) = \sigma _{A,A} (x)
\]
will be used.

For any sets \(A,B\), define the Additive Energy \(E(A,B)\) and Multiplicative Energy \(M(A,B)\) by
\[
    E(A,B) = \left\lvert \left\{ (a_1,a_2,b_1,b_2) \in A^{2} \times B^{2} : a_1 - b_1 = a_2 - b_2 \right\}  \right\rvert 
\]
and
\[
    M(A,B) = \left\lvert \left\{ (a_1,a_2,b_1,b_2)\in  A^{2} \times B^{2} : \frac{a_1}{b_1}  = \frac{a_2}{b_2}  \right\}  \right\rvert
.\]

Observe that
\[
    E(A,B) = \sum _{x \in A - B} \delta_{A,B} (x)^{2}
\]
and
\[
    M(A,B) = \sum _{x \in \frac{A}{B} } r_{\frac{A}{B} } (x)^{2} 
.\]

This definition is symmetric in the sense that a 4-tuple \((a_1,a_2,b_1,b_2)\in A^{2} \times B^{2}\) is a solution to
\[
    a_1 - b_1 = a_2 - b_2
\]
if and only if it is a solution to
\[
    a_1+b_2 = a_2+b_1
,\]
and therefore
\[
    E(A,B) =\sum _{x \in A - B} \delta_{A,B}  (x)^{2} = \sum _{x \in A + B} \sigma_{A,B}  (x)^{2}
.\]

There is a similar argument for multiplicative energy.
Any 4-tuple \((a_1,a_2,b_1,b_2) \in A^{2} \times B^{2}\) with nonzero entries is a solution to
\[
    \frac{a_1}{b_1} = \frac{a_2}{b_2}
\]
if and only if it is a solution to
\[
    a_1 b_2 = a_2 b_1
.\]
There are at most
\[
    \sum _{i = 1} ^{4} \binom{4}{i}  = 15
\]
4-tuples with zero entries, so
\[
    M(A,B) = \sum _{x \in \frac{A}{B} } r_{\frac{A}{B} } (x) ^{2} \asymp \sum _{x \in AB} r_{AB} (x)^{2}
.\]

We also define higher energies
\[
    E_{n} (A,B) = \sum _{x \in A-B} \delta_{A,B} (x)^{n}  
,\]
so
\[
    E(A) = E_{2} (A)
,\]
and as a shorthand use
\[
    E_{n} (A) = E_{n} (A,A)
.\]
Similar definitions and shorthand are used for multiplicative energy.

We can relate the energies to the sizes of the sum and product sets by the Cauchy-Schwarz Inequality.
\[
    \left\lvert A \right\rvert \left\lvert B \right\rvert = \sum _{x \in A + B} \sigma_{A,B}  (x) \leq \left\lvert A + B \right\rvert ^{\frac{1}{2} } E(A,B)^{\frac{1}{2} }
,\]
and
\[
    \left\lvert A \right\rvert \left\lvert B \right\rvert = \sum _{x \in AB} r_{AB} (x) \leq \left\lvert AB \right\rvert ^{\frac{1}{2} }M(A,B)^{\frac{1}{2} }
.\]
Similar inequalities can be derived for \(\left\lvert A-B \right\rvert \) and \(\left\lvert \frac{A}{B}  \right\rvert \).

Let \(I \subset \mathbb{R} \) be an interval. 
    We call a function \(f: I \to \mathbb{R} \) convex if for
    any 2 points \(x_1,x_2 \in I\), with \(x_1 \neq x_2\), and any \(\lambda \in (0,1)\),
    \[
        f(\lambda x_1 + (1-\lambda)x_2) < \lambda f(x_1) + (1-\lambda)f(x_2)
    .\]

    A finite set \(A \subset \mathbb{R}\) is convex if there is a function \(f: [1, \left\lvert A \right\rvert ] \to \mathbb{R} \)
such that
\[
    A = \left\{ f(i) : i \in \left\{ 1, \cdots , \left\lvert A \right\rvert  \right\}  \right\} 
.\]

A property of convex functions which will come up later, and is worth proving now, is
\begin{lemma}
Let \(I \subset \mathbb{R} \) be an interval. Let \(f : I \to \mathbb{R} \) be convex. Let \(T \subset \mathbb{R} ^{2}\).

Take \(\ell  = \left\{ (x,f(x)) : x \in I \right\} \) to be the graph of \(f\). We have that
\[
    \forall t_1,t_2 \in T ~,~ \left\lvert \left( \ell + t_1 \right) \cap \left( \ell +  t_2 \right)  \right\rvert   \leq  1
.\]

That is that translations of the graph of a convex function intersect in at most one point.
\end{lemma}

\begin{proof}
We first show that for any \(x_1, x_2\) with \(x_1 < x_2\) and \(t > 0\), we have
\[
    f(x_1 + t) - f(x_1) < f(x_2 + t) - f(x_2)
.\]

Take \(\lambda_1 = \frac{x_1 - x_2}{x_1 -x_2 -t} \) to get the result
\[
    f(x_1 + t)  = f(\lambda_1 x_1 + \left( 1- \lambda_1 \right) \left( x_2 + t \right) ) < \lambda_1 f(x_1) + \left( 1 - \lambda_1 \right) f(x_2 + t)
.\]

Take \(\lambda_2 = \frac{-t}{x_1-x_2-t} \) to get the result
\[
    f(x_2) = f(\lambda_2x_1 + \left( 1 - \lambda_2 \right) \left( x_2 + t \right) ) < \lambda_2 f(x_1) + \left( 1- \lambda_2 \right) f(x_2+ t) 
.\]

Observing that \(\lambda_1 = 1-\lambda_2\) and adding the inequalities we get
\[
    f(x_1 + t) + f(x_2) < f(x_1) + f(x_2 + t)
,\]
or
\[
    f(x_1 + t) - f(x_1) < f(x_2 + t) - f(x_2)
.\]

Without loss of generality, we can consider \(\ell\) and a single translation \(t_0 = \left( t_1,t_2 \right) \).
A point of intersection between these curves is a solution to the equation
\[
    (x_1, f(x_1)) = (x_2 + t_1, f(x_2) + t_2)
,\]
or
\[
\begin{cases}
x_1 = x_2 + t_1 \\
f(x_1) = f(x_2) + t_2
\end{cases}
,\]
or
\[
    f(x_2 + t_1) = f(x_2) + t_2
.\]
A second point of intersection is a solution to
\[
    f(x_3 + t_1) = f(x_3) + t_2
\]
where \(x_3 \neq x_2\).

Adding these equations, we see that 2 points of intersection can occur only if
\[
    f(x_2 + t_1) - f(x_2) = f(x_3 + t_1)- f(x_3)
\]
which yields a contradiction.

\end{proof}

A function \(f: I \to \mathbb{R} \) is concave iff \(-f\) is convex.
An important symmetry to notice is that concave sets \(A\), defined by a 
concave function \(f\) satisfy
\[
    \left\lvert A + A \right\rvert = \left\lvert \left( -A \right) + \left( -A \right)  \right\rvert 
.\]
That is, any results throughout the rest of this report concerning the size of convex
sets also hold for concave sets.

The final tool we'll introduce is dyadic partitioning. Let \(S \subset \mathbb{R} \) be a finite set, \(f: S \to \mathbb{R} \) be a function,
and \(M\) be the maximum value of \(f(x)\) for \(x \in S\). We partition a sum of
\(f(x)\) in the following way
\[
    \sum _{x \in S} f(x) = \sum _{j \leq \log _{2} \left( M \right)} \sum _{\substack{ x \in S \\ 2^{j-1} \leq f(x) < 2^{j} }} f(x)
.\]
Out of the \(\log _{2} \left( M \right) \) partitions of the sum, one of them must be the largest, so
\[
    \sum _{x \in S} f(x) \leq  \log _{2} \left( M \right)  \sum _{\substack{ x \in S \\ 2^{k-1} \leq f(x) < 2^{k} }} f(x)
\]
for some \(k \leq \log _{2} \left( M \right) \).

We use this notation introduce the technique. Throughout this report, we write
\(\Delta= 2^{k-1}\), and
\[
    D =\left\{ x \in S : f(x) \asymp \Delta\right\} 
\]
so that
\[
    \sum _{x \in S} f(x) \ll \log \left( M \right) \sum _{x \in D} f(x)\asymp \log \left( M \right) \Delta \left\lvert D \right\rvert 
.\]

This technique is useful in this problem because a factor of \(\log \left( M \right) \) will
be negligible in most circumstances. This allows us to sum only over \(x\) for which \(f(x)\) is
a particular order and use this to find an upper bound.


%────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
%────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────


\section{Statement of Problems}

We'll now move on to precise statements of the mentioned problems, and statements of the most modern results.

The idea that there does not exist a set with a small sum and product set is
stated precisely as
\begin{conjecture}[Sum-Product Conjecture]
For every finite set \(A \subset \mathbb{R} \),
\[
    \max \left( \left\lvert A+A \right\rvert, \left\lvert A \cdot A \right\rvert  \right) \gtrsim   \left\lvert A \right\rvert^{2}
\]
\end{conjecture}

The idea that convexity opposes additive structure is stated precisely as

\begin{conjecture}[Convex Sumset Conjecture]
    For finite and convex set \(A\),
    \[
        \left\lvert A+A \right\rvert \gtrsim   \left\lvert A \right\rvert ^{2}
    \]
\end{conjecture}

Both of these conjectures are sharp in the sense that the power of log is not negligible. In the appendix I prove that
\[
    \left\lvert \log \left( [n] \right) + \log \left( [n] \right)  \right\rvert = \left\lvert [n]\cdot [n] \right\rvert = o (n^{2})
.\]

To date, the best results for both of these conjectures are proven in \cite{stevens-rudnev}.
\begin{theorem}
    For finite sets \(A \subset \mathbb{R} \),
    \[
        \max \left( \left\lvert A+A \right\rvert ~,~ \left\lvert A \cdot A \right\rvert  \right) \gtrsim    \left\lvert A \right\rvert^{\frac{4}{3} + \frac{2}{1167}}
    \]
\end{theorem}
and
\begin{theorem}
    For finite and convex sets \(A \subset \mathbb{R} \),
    \[
        \left\lvert A+A \right\rvert \gtrsim   \left\lvert A \right\rvert ^{\frac{30}{19} }
    \]
\end{theorem}

These results will not be covered in this report. In this report,
the strongest results proven are
\begin{theorem}\label{thm:solymosi-sp}
For finite sets \(A \subset \mathbb{R} \),
\[
    \max \left( \left\lvert A+A \right\rvert , \left\lvert AA \right\rvert  \right) \gtrsim \left\lvert A \right\rvert ^{\frac{4}{3} }
.\]
\end{theorem}
\begin{theorem}\label{thm:shkredov-sum}
For finite and convex sets \(A \subset \mathbb{R} \),
\[
    \left\lvert A + A \right\rvert \gtrsim  \left\lvert A \right\rvert ^{\frac{20}{13}  }
.\]
\end{theorem}
and
\begin{theorem}\label{thm:shkredov-diff}
For finite and convex sets \(A \subset \mathbb{R} \),
\[
    \left\lvert A - A \right\rvert \gtrsim \left\lvert A \right\rvert ^{\frac{8}{5} }
.\]
\end{theorem}

Theorem \ref{thm:solymosi-sp} is the weakest amongst these results, although it is
still near the best known result.
Theorem \ref{thm:shkredov-diff} gives the best known exponent on \(\left\lvert A \right\rvert \), and
Theorem \ref{thm:shkredov-sum} is obtained by a slight variation on the same methods.

\section{Graphs and the Crossing Number Inequality}

A profoundly useful theorem in the study of sum-product conjectures is the
Szemeredi-Trotter theorem, a statement about systems of points and lines.

The easiest way to prove this theorem is through the use of the crossing number inequality,
a statement about how planar a given graph can be. The purpose of this section is to give a proof
of the crossing number inequality. Due to the brevity of this section, the definitions and proofs given will not be
as rigorous as they should be.

The first paper to employ the crossing number inequality to proving the Szemeredi-Trotter theorem is
\cite{szekely-SzT}. This argument was largely taken from \cite{tao-blog-CRI}.

There are pictures included to help illustrate some ideas. 

A graph \(G\) is a pair \(G = (V,E)\) where each \(e \in E\) is of the form \(e \subset V\) with \(\left\lvert e \right\rvert = 2\).
We call the set \(V\) the vertices, and the set \(E\) the edges. A drawing is
a representation of a graph with vertices as points in the plane and edges as curves between their
respective vertices. For example:

\begin{figure}[h]
    \centering
    \includegraphics[width=0.1\textwidth]{3graph.png}
    \caption{A drawing of a graph with \(V = \left\{ A,B,C \right\}\) and \(E = \left\{ \left\{ A,B \right\} , \left\{ B,C \right\} ,\left\{ A,C \right\}  \right\} \)}
\end{figure}

A graph is connected if for any 2 vertices, there exists a sequence of edges which join them.
Note that connectedness is a property of a graph and not of the drawing of a graph.

There are infinitely many ways to draw any given graph. A crossing in a drawing
of a graph is an intersection between 2 curves which represent edges. The 
crossing number of a graph is the minimum number of crossings a drawing
of the graph can have. Denote this by
\(\crossing(G)\). A graph \(G\) is called planar if its crossing number is 0.

A precise statement of the Crossing Number Inequality is

\begin{theorem}\label{thm:crossing-number-inequality}
    Let \(G = (V,E)\) be a connected graph. If \(\left\lvert E \right\rvert \geq 4 \left\lvert V \right\rvert \)
    then
    \[
        \crossing\left( G \right) \gg \frac{\left\lvert E \right\rvert ^{3} }{\left\lvert V \right\rvert ^{2}}  
    .\]
\end{theorem}

For a drawing of a planar graph, we call any region of the plane which is bounded by edges a face. We also call
the unbounded region of the plane a face. Here is an example of a drawing of a planar
graph with labelled faces:

\begin{figure}[h]
    \centering
    \includegraphics[width=0.4\textwidth]{faceimage.png}
    \caption{Drawing of Planar Graph with Labeled Faces \(f_{i} \)}
\end{figure}

Observe that any non-planar graph \(G = (V,E)\) can be turned into a planar graph by removing at most
\(\crossing\left( G \right) \) edges from \(E\). Therefore, a bound on the number of
edges a graph can have and remain planar yields a bound on the crossing number for any graph.
A famous theorem relating the vertices and edges of planar graphs is

\begin{theorem}[Euler's Formula for Planar Graphs]\label{thm:euler-formula-graphs}
Let \(G = (V,E)\) be a connected planar graph, with \(\left\lvert V \right\rvert \geq 1\), and consider some drawing with 0 crossings.
Let \(F\) be the set of all faces of this drawing.
\[
    \left\lvert V \right\rvert - \left\lvert E \right\rvert + \left\lvert F \right\rvert = 2
.\]
\end{theorem}

\begin{proof}
We prove this by induction on the number of edges. For the base case, \(\left\lvert E \right\rvert = 0\),
\(G\) consists of a single vertex and a single face, so
\[
    \left\lvert V \right\rvert - \left\lvert E \right\rvert + \left\lvert F \right\rvert =2
.\]
Now suppose that this equality holds for all graphs with no more than \(e - 1\) edges,
and consider a graph with \(e\) edges. 

If \(G\) contains no cycles, there
is only one face, so we may remove a vertex and a corresponding edge, which results
in a graph with \(e-1\) edges satisfying Euler's formula. Because we removed one
vertex and one edge, the original graph also satisfies Euler's formula.

The next case is \(G\) containing at least one cycle. If this is the case, we
may remove an edge from the cycle, thereby decreasing the amount of faces by one.
The remaining graph satisfies Euler's formula, and therefore the original graph does too.

By induction on the number of edges, Euler's formula holds for all connected planar graphs.
\end{proof}

The dependence on \(\left\lvert F \right\rvert \) in Euler's formula can be removed by using
its obvious dependence on \(\left\lvert E \right\rvert \). 

We call an edge incident to a face if the edge is one of the bounding edges which define the face. 
Define \(\chi : F \times E \to \left\{ 0,1 \right\} \) as the incidence function, so
\(\chi(f,e) = 1\) if \(f\) and \(e\) are incident, and \(\chi(f,e) = 0\) otherwise.
The total number of face edge incidences is
\[
    I(F,E) = \sum_{f\in F} \sum _{e \in E} \chi(f,e)
.\]

Because the crossing number inequality is a statement about order of magnitude,
we may assume \(\left\lvert E \right\rvert  \geq 3\), so that every face is incident to at least 3 edges.
It follows that
\[
    I \geq \sum _{f \in F} 3 = 3 \left\lvert F \right\rvert 
.\]
Every edge is incident to at most 2 faces, so it follows that
\[
    I \leq \sum _{e \in E} 2 = 2 \left\lvert E \right\rvert 
.\]
Therefore
\[
    3 \left\lvert F \right\rvert \leq 2 \left\lvert E \right\rvert 
\]
or
\[
    \left\lvert F \right\rvert \leq \frac{2}{3} \left\lvert E \right\rvert 
.\]
Applying this to Euler's formula,
\[
    \left\lvert V \right\rvert - \left\lvert E \right\rvert + \frac{2}{3} \left\lvert E \right\rvert \geq 2
\]
or
\[
    \left\lvert E \right\rvert \leq 3 \left\lvert V \right\rvert -6
\]
when \(\left\lvert E \right\rvert \geq 3\).
Now suppose that \(G = (V,E)\) is non-planar and connected. As mentioned before, \(G\) may be turned planar by removing
at most \(\crossing\left( G \right) \) edges.
Therefore, for any graph \(G\) with \(\left\lvert E \right\rvert  \geq 3\),
\[
    \left\lvert E \right\rvert - \crossing\left( G \right) \leq 3 \left\lvert V \right\rvert - 6
\]
or
\[
    \crossing\left( G \right) > \left\lvert E \right\rvert - 3 \left\lvert V \right\rvert 
.\]

To further improve this inequality, we apply the probabilistic method
to the deletion of vertices of \(G\).

Let each \(v \in V\) be
removed with a probability \(1-p~,~  p \in (0,1)\). Let the remaining set of vertices be \(V'\). 

An edge is
removed whenever either of the corresponding vertices are removed. Let the remaining set of edges be \(E'\). Let the remaining graph be
\(G' = (V',E')\). We have that if \(\left\lvert E' \right\rvert \geq 3\),
\[
    \crossing\left( G' \right) \geq \left\lvert E' \right\rvert - 3 \left\lvert V' \right\rvert 
,\]
and so
\[
    \mathbb{E} \left( \crossing\left( G' \right)  \right) \geq \mathbb{E} \left( \left\lvert E' \right\rvert - 3 \left\lvert V' \right\rvert  \right) 
,\]
or, by the linearity of the expected value,
\[
    \mathbb{E} \left( \crossing\left( G' \right)  \right) \geq \mathbb{E} \left( \left\lvert E' \right\rvert  \right) - 3 \mathbb{E} \left( \left\lvert V' \right\rvert  \right)    
.\]
Each \(v \in V\) is removed with probability \(1-p\), so
\[
    \mathbb{E} \left( \left\lvert V' \right\rvert  \right) = p \left\lvert V \right\rvert 
.\]
Each edge remains only when both corresponding vertices remain. Each vertex remains
independently with a probability \(p\), so 
\[
    \mathbb{E} \left( \left\lvert E' \right\rvert  \right) = p^{2}\left\lvert E \right\rvert 
.\]

We can bound \(\mathbb{E} \left( \crossing\left( G' \right)  \right) \) by considering a drawing of \(G\) with the minimum number of crossings. 
Each crossing remains and only when both corresponding edges remain, each of which occurs independently with probability
\(p^{2}\).

The expected value of the number of crossings remaining in the drawing is \(p^{4} \crossing\left( G \right) \).
There is no guarantee that this drawing is optimal to minimize the crossings of \(G'\), but we may conclude that
\[
    \mathbb{E} \left( \crossing\left( G' \right)  \right) \leq p ^{4} \crossing\left( G \right)
,\]
and therefore that
\[
    p^{4} \crossing\left( G \right) \geq \mathbb{E} \left( \crossing\left( G \right)  \right) \geq p^{2} \left\lvert E \right\rvert - 3 p \left\lvert V \right\rvert 
\]
for any \(p \in (0,1)\). 


Because we are only concerned with an order of magnitude bound, we assume \(\left\lvert E \right\rvert \geq 4\left\lvert V \right\rvert \), and take \(p = \frac{4\left\lvert V \right\rvert }{\left\lvert E \right\rvert }\).
This yields the result
\[
    \crossing\left( G \right) \geq \frac{\left\lvert E \right\rvert }{\left( \frac{4 \left\lvert V \right\rvert }{\left\lvert E \right\rvert }   \right)^{2} } - \frac{3 \left\lvert V \right\rvert }{\left( \frac{4 \left\lvert V \right\rvert }{\left\lvert E \right\rvert }  \right) ^{3} } = \frac{1}{16} \left( \frac{\left\lvert E \right\rvert ^{3} }{\left\lvert V \right\rvert ^{2}} - \frac{3 \left\lvert E \right\rvert ^{3} }{4 \left\lvert V \right\rvert ^{2}}  \right) \gg \frac{\left\lvert E \right\rvert ^{3} }{\left\lvert V \right\rvert ^{2}}
.\]

\section{The Szemeredi-Trotter Theorem}

With this, we can employ the argument of \cite{szekely-SzT} to prove the Szemeredi-Trotter Theorem.
A precise statement of the Szemeredi-Trotter Theorem, first proven in \cite{SzT-original} is

\begin{theorem}[Szemeredi-Trotter Theorem]
Let \(P \subset \mathbb{R} ^{2}\) be a finite set of points. Let \(\mathcal{L} \) be a finite set of curves in \(\mathbb{R} ^{2}\). 

Let \(\chi : P \times \mathcal{L}  \to \left\{ 0,1 \right\} \) be the incidence function
between a point and a line, so
\[
\chi(p,l) = 
\begin{cases}
1 ~ ~ \text{if} ~ ~ p\in l\\
0 ~ ~ \text{otherwise} 
\end{cases}
\]

If any two \( l \in \mathcal{L} \) intersect in at most one point,
then the total number of point-curve incidences,
\[
    I(P, \mathcal{L} ) = \sum _{(p,l)\in P \times \mathcal{L} } \chi(p,l)
\]
satisfies
\[
    I(P,\mathcal{L} ) \ll  \left\lvert P \right\rvert^{\frac{2}{3} } \left\lvert \mathcal{L}  \right\rvert ^{\frac{2}{3} } + \left\lvert P \right\rvert + \left\lvert \mathcal{L}  \right\rvert 
.\]
\end{theorem}

\begin{proof}
This is proven by turning a system of curves and points into a graph. We first omit all points and curves which
contribute to one or fewer incidences. 

For each remaining curve, if there are \(n\) incidences
along it, we partition it into \(n-1\) curves, each with 2 incidences. These become
the edges of the drawing of some graph. Let the set of edges be \(E\).

If \(I_0(P , \mathcal{L} )\) is the remaining number of incidences,
\[
    \left\lvert E \right\rvert \geq  I_0(P , \mathcal{L} ) - \left\lvert \mathcal{L}  \right\rvert 
.\]

Let the vertices of the graph, \(V\), be the remaining points in the system. Obviously, \(\left\lvert V \right\rvert \leq \left\lvert P \right\rvert \).

Because any two curves intersect in at most one point, \(\crossing\left( G \right) \) is at most
\(\left\lvert \mathcal{L}  \right\rvert ^{2}\).

Supposing that \(\left\lvert E \right\rvert \geq 4 \left\lvert V \right\rvert \), we have that
\[
    \left\lvert \mathcal{L}  \right\rvert ^{2} \geq \crossing\left( G \right) \gg \frac{\left( I_0 \left( P, \mathcal{L}  \right) - \left\lvert \mathcal{L}  \right\rvert \right)  ^{3}  }{\left\lvert P \right\rvert ^{2}} 
\]
or
\[
    I_0(P , \mathcal{L} ) \ll \left\lvert \mathcal{L}  \right\rvert ^{\frac{2}{3} } \left\lvert P \right\rvert ^{\frac{2}{3} } + \left\lvert \mathcal{L}  \right\rvert 
.\]

If \(\left\lvert V \right\rvert \geq \frac{1}{4} \left\lvert E \right\rvert \), then \(\left\lvert V \right\rvert  \gg \left\lvert E \right\rvert \), or
\[
    \left\lvert P \right\rvert \gg I_0\left( P , \mathcal{L}  \right) - \left\lvert \mathcal{L}  \right\rvert \implies I_0(P,\mathcal{L} ) \ll \left\lvert P \right\rvert  + \left\lvert \mathcal{L}  \right\rvert  
.\]

Finally, the remaining incidences not yet counted is 
\[
    I(P, \mathcal{L} ) - I_0 (P , \mathcal{L} ) \ll \left\lvert \mathcal{L} \right\rvert + \left\lvert P \right\rvert
,\]
so
\[
    I(P, \mathcal{L} ) \ll \left\lvert P \right\rvert ^{\frac{2}{3} }\left\lvert \mathcal{L}  \right\rvert ^{\frac{2}{3} } + \left\lvert P \right\rvert + \left\lvert \mathcal{L}  \right\rvert 
.\]
\end{proof}

The Szemeredi Trotter theorem has a handful of direct applications to the sum
product conjecture and the convex sumset conjecture. The main theorem which leads
to these results is proven in \cite{elekes}.

\begin{theorem} \label{thm:translations-of-convex-szt}
Let \(A \subset \mathbb{R} \) be finite, with \(\left\lvert A \right\rvert = n\). 

Label the
elements of \(A\) so that \(a_1 < a_2 < \cdots < a_{n} \). 

Let \(f : [a_1,a_{n} ] \to \mathbb{R} \) be convex. Let \(S = \left\{ (a,f(a)) : a \in A \right\} \) and \(T \subset \mathbb{R} ^{2}\) be finite. 

We have
\[
    \left\lvert S + T \right\rvert \gg \max \left( \left\lvert S \right\rvert ^{\frac{3}{2} } \left\lvert T \right\rvert ^{\frac{1}{2} } , \left\lvert S \right\rvert \left\lvert T \right\rvert  \right)
.\]
\end{theorem}

\begin{proof}
    Let
    \[
        L_{t} = \left\{ (x,f(x)) + t : x \in [a_1,a_{n} ], t \in T \right\} 
    ,\]
    and let
    \[
        \mathcal{L} = \left\{ L_{t} : t \in T \right\} 
    .\]
    For every \(x \in A\), \((x,f(x)) + t \in S + T\). Therefore, there are \(\left\lvert A \right\rvert = \left\lvert S \right\rvert \) incidences
    between \(L_{t} \) and the point set \(S + T\), for all \(t \in T\). It follows that there are \(\left\lvert S \right\rvert \left\lvert T \right\rvert \)
    total incidences. The set \(\mathcal{L} \) consists of
    translations of the graph of a convex function, so the Szemeredi-Trotter theorem is satisfied. Thus
    
    \[
        \left\lvert S \right\rvert \left\lvert T \right\rvert \ll \left\lvert S + T \right\rvert ^{\frac{2}{3} } \left\lvert T \right\rvert ^{\frac{2}{3} } + \left\lvert S + T \right\rvert + \left\lvert T \right\rvert 
    .\]
    Trivially,
    \[
        \left\lvert S + T \right\rvert \geq \left\lvert T \right\rvert 
    ,\]
    so
    \[
        \left\lvert S \right\rvert \left\lvert T \right\rvert  \ll\max \left( \left\lvert S + T \right\rvert ^{\frac{2}{3} }\left\lvert T \right\rvert ^{\frac{2}{3} }, \left\lvert S+T \right\rvert  \right) 
    ,\]
    or
    \[
        \left\lvert S + T \right\rvert \gg \max \left( \left\lvert S \right\rvert ^{\frac{3}{2} } \left\lvert T \right\rvert ^{\frac{1}{2} }, \left\lvert S \right\rvert \left\lvert T \right\rvert  \right) 
    .\]
\end{proof}

The following corollaries are also proven in \cite{elekes}.

\begin{corollary}
    For convex and finite sets \(A \subset \mathbb{R} \), and finite sets \(B \subset \mathbb{R} \),
    \[
        \left\lvert A + B \right\rvert \gg \left\lvert A \right\rvert \left\lvert B \right\rvert ^{\frac{1}{2} }
    .\]
\end{corollary}
    
\begin{proof}
Let \(A \subset \mathbb{R} \) be finite. Let \(n = \left\lvert A \right\rvert \). Let \(f : [1,n] \to \mathbb{R} \) be the
convex function for which
\[
    A = \left\{ f(i) : i \in [n] \right\} 
.\]
Take
\[
    S = \left\{ (i,f(i)) : i \in [n]\right\} 
\]
and
\[
    T = [n] \times B
.\]
We have
\[
    S + T \subset \left( [n] + [n] \right)  \times \left( A + B \right) 
,\]
so
\[
    \left\lvert S + T \right\rvert \leq \left\lvert [n] + [n] \right\rvert \left\lvert A + B \right\rvert \ll \left\lvert A \right\rvert \left\lvert A + B \right\rvert 
.\]
Apply Theorem \ref{thm:translations-of-convex-szt} to get
\[
    \left\lvert A \right\rvert \left\lvert A + B \right\rvert \gg \left\lvert S + T \right\rvert \gg \max \left( \left\lvert A \right\rvert ^{\frac{3}{2} }\left( \left\lvert A \right\rvert \left\lvert B \right\rvert  \right) ^{\frac{1}{2} }, \left\lvert A \right\rvert \left( \left\lvert A \right\rvert \left\lvert B \right\rvert  \right)  \right) 
\]
or
\[
    \left\lvert A \right\rvert \left\lvert A + B \right\rvert \gg \left\lvert A \right\rvert ^{2}\left\lvert B \right\rvert ^{\frac{1}{2} } \implies  \left\lvert A + B \right\rvert \gg \left\lvert A \right\rvert \left\lvert B \right\rvert ^{\frac{1}{2} }
.\]
\end{proof}

This will be used to achieve stronger results later, but in particular, this gives the result
\begin{corollary}
For finite and convex sets \(A \subset \mathbb{R} \),
\[
    \left\lvert A+A \right\rvert \gg \left\lvert A \right\rvert ^{\frac{3}{2} }
.\]
\end{corollary}

We can also use Theorem \ref{thm:translations-of-convex-szt} to get a result
on the Sum-Product conjecture.
\begin{corollary}
    For finite sets \(A \subset \mathbb{R}^{+} \) of positive real numbers,
    \[
        \max \left( \left\lvert A+A \right\rvert, \left\lvert A\cdot A \right\rvert  \right) \gg \left\lvert A \right\rvert ^{\frac{5}{4} }
    .\]
\end{corollary}

\begin{proof}
Let \(A \subset \mathbb{R} \) be finite. 

Label the elements of \(A\) so that \(a_1 < \cdots < a_{n} \).
Let \(f : [a_1,a_{n} ] \to \mathbb{R} \) be concave or convex.
Take
\[
    S = \left\{ (a,f(a)) : a \in A \right\} 
,\]
and
\[
    T = A \times f(A)
.\]
Observe that
\[
    S + T \subset \left( A + A \right) \times \left( f(A) + f(A) \right) 
,\]
so that
\[
    \left\lvert S + T \right\rvert \ll \left\lvert A + A \right\rvert \left\lvert f(A) + f(A) \right\rvert
.\]

Apply Theorem \ref{thm:translations-of-convex-szt} to get
\[
    \left\lvert A+A \right\rvert \left\lvert f(A) + f(A) \right\rvert \gg \left\lvert S+T \right\rvert \gg \max \left( \left\lvert A \right\rvert ^{\frac{3}{2} } \left( \left\lvert A \right\rvert ^{2} \right) ^{\frac{1}{2} }, \left\lvert A \right\rvert \left\lvert A \right\rvert ^{2} \right) 
\]
or
\[
    \max \left( \left\lvert A + A \right\rvert , \left\lvert f(A) + f(A) \right\rvert  \right) \gg \left\lvert A \right\rvert ^{\frac{5}{4} }
\]
for any convex or concave function \(f : [a_1,a_{n} ] \to \mathbb{R}\).

If \(A \subset \mathbb{R} ^{+}\), we may take \(f(x) = \log \left( x \right) \), immediately yielding the desired result.
\end{proof}

The Szemeredi-Trotter theorem also provides more general tools which can be
used alongside other results to sharpen the above bounds.

\begin{theorem}\label{shkredov-lemma-szt}

    Let \(A \subset \mathbb{R} \) be convex, then for every 
    finite set \(B \subset \mathbb{R} \) we have

    \[
        \left\lvert \left\{ x \in A-B : \delta_{A,B} (x) \geq \tau \right\}  \right\rvert \ll \frac{\left\lvert A \right\rvert \left\lvert B \right\rvert^{2} }{\tau^{3} }
    .\]
\end{theorem}

\begin{proof}
Let \(\left\lvert A \right\rvert  = n\). Let \(f: [1,n] \to \mathbb{R} \) be the convex
function defining the set \(A\). Take
\[
    \ell _{a,b} = \left\{ \left( x,f(x) \right) + (a,b) : x \in [1,n] \right\} 
\]
and
\[
    \mathcal{L} = \left\{ \ell _{a,b} : a \in [n] , b \in -B \right\} 
.\]
Take
\[
    P = \left( [n] + [n]  \right) \times \left( A - B \right) 
,\]
and let \(P_{\tau}  \subset P\) be the largest subset of \(P\) for which every
point has at least \(\tau\) lines passing through it. 

For some \(y\)-axis ordinate, if \(\delta_{A} (y) \geq \tau\), there are at least
\(\left\lvert A \right\rvert \) many points \(p \in P_{\tau} \) with the same
\(y\) ordinate. Therefore,
\[
   \left\lvert  \left\{ x \in A -B : \delta_{A} (x) \geq \tau \right\}  \right\rvert \leq \frac{\left\lvert P_{\tau}  \right\rvert }{\left\lvert A \right\rvert } 
.\]

It is clear that
\[
    I(P_{\tau} , \mathcal{L} ) \geq \tau \left\lvert P_{\tau}  \right\rvert 
,\]
so by the Szemeredi-Trotter theorem
\[
    \tau\left\lvert P_{\tau}  \right\rvert \ll \left\lvert P_{\tau}  \right\rvert ^{\frac{2}{3} } \left\lvert \mathcal{L}  \right\rvert ^{\frac{2}{3} } + \left\lvert P_{\tau}  \right\rvert + \left\lvert \mathcal{L}  \right\rvert \ll \left\lvert P_{\tau}  \right\rvert ^{\frac{2}{3} } \left\lvert \mathcal{L}  \right\rvert ^{\frac{2}{3} } 
.\]
It follows that
\[
    \left\lvert P_{\tau}  \right\rvert \ll \frac{\left\lvert A \right\rvert ^{2} \left\lvert B \right\rvert ^{2} }{\tau^{3} } 
,\]
or
\[
    \left\lvert  \left\{ x \in A -B : \delta_{A} (x) \geq \tau \right\}  \right\rvert \ll \frac{\left\lvert A \right\rvert \left\lvert B \right\rvert ^{2} }{\tau^{3} } 
.\]
\end{proof}

An immediate corollary of this is

\begin{corollary}
Let \(A \subset \mathbb{R} \) be a convex and finite set. Let \(B \subset \mathbb{R} \) be finite.
Order elements \(s_{i} \in A-B\) such that 
\[
    \delta_{A,B} (s_1) \geq \delta_{A,B} (s_2) \geq \cdots \geq \delta_{A,B} (s _{\left\lvert A - B \right\rvert } )
.\]
For every \(1 \leq r \leq \left\lvert A - B \right\rvert\) we have
\[
    \delta_{A,B} (s _{r} ) \ll \frac{\left\lvert A \right\rvert^{\frac{1}{3} } \left\lvert B \right\rvert ^{\frac{2}{3} } }{r^{\frac{1}{3} }}
.\]
\end{corollary}

\begin{proof}
\[
    r = \left\lvert \left\{ x \in A-B : \delta _{A,B} (x) \geq \delta_{A,B} (s _{r} ) \right\}  \right\rvert \ll \frac{\left\lvert A \right\rvert \left\lvert B \right\rvert ^{2} }{\delta_{A,B} (s _{r} )^{3} } \implies \delta_{A} (s _{r} ) \ll \frac{\left\lvert A \right\rvert ^{\frac{1}{3} } \left\lvert B \right\rvert ^{\frac{2}{3} }}{r^{\frac{1}{3} }}
.\]
\end{proof}

Later in the next section we'll use these results to find bounds on the additive
energies between certain sets, but first we'll introduce some simpler results which
use additive and multiplicative energy to show why such bounds are useful.

\section{Additive and Multiplicative Energy Estimates}

Recall that
\[
    \left\lvert A + A \right\rvert \geq  \frac{\left\lvert A \right\rvert ^{4}}{ E(A) }
\]
and
\[
    \left\lvert AA \right\rvert \geq \frac{\left\lvert A \right\rvert ^{4}}{M(A) }
.\]

Observe that finding an upper bound on \(E(A)\) or \(M(A)\) in terms of \(\left\lvert A + A \right\rvert, \left\lvert AA \right\rvert    \), and \(\left\lvert A \right\rvert \)
yields a sum product theorem. In this section we showcase a result which employs this idea.
We also give other, more complicated, estimates on additive energies involving a convex set which
will be used later.

The aforementioned result is Theorem \ref{thm:solymosi-sp}, found in \cite{Solymosi}. It gives a stronger result
on the sum-product conjecture than the Szemeredi-Trotter theorem. 
\begin{theorem*}
Let \(A \subset \mathbb{R}^{+} \) be finite.
\[
    \max \left( \left\lvert A+A \right\rvert , \left\lvert AA \right\rvert  \right) \gtrsim  \left\lvert A \right\rvert ^{\frac{4}{3}}
.\]
\end{theorem*}

\begin{proof}

We begin with a construction. Consider the set \(A^{2}\), along with the smallest set of lines
through the origin which cover \(A^{2}\).

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{Sol1.png}
    \caption{Example with \(A = \left\{ 1,2,4,8 \right\} \).}
\end{figure}

The claim is that each line represents an element of \(\frac{A}{A} \). This is
easy to see, two pairs \((a_1,a_2),(b_1,b_2) \in A^{2}\) give the same representation as a quotient if
and only if
\[
    \frac{a_2}{a_1} = \frac{b_2}{b_1} 
.\]
Observe that this is the slope of the line through the origin and the points \((a_1,a_2), (b_1,b_2)\).
This shows that the number of lines in the construction is \(\left\lvert \frac{A}{A}  \right\rvert \), the slope
of each line is an element in \(\frac{A}{A} \), and the number of points on a line
of slope \(m\) is the number of representations of \(m\) as a quotient, \(r_{\frac{A}{A} } (m)\).

We'll now prove 2 facts about the set of vector sums of 2 points lying on consecutive lines.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{sol2.png}
    \caption{Vector sums of 2 points lying on consecutive lines. Figure taken from \cite{Solymosi}, where the argument was first given.}
    \label{fig:solymosi}
\end{figure}

Firstly, that the set of vector sums of points along either line is disjoint for each
choice of consecutive lines. In figure \ref{fig:solymosi}, this is represented by the blue arrow between lines
\(l _{j} , l _{j + 1} \) and \(l _{k}  , l _{k+1} \).
To show this, consider 2 consecutive lines and the set of all vector sums between a point on each line.
If our points are \(\left( a_1,a_2 \right)\) and \(\left( b_1,b_2 \right) \), with
\[
    \frac{a_2}{a_1} > \frac{b_2}{b_1}
,\]
then the slope of their sum is
\[
    \frac{a_2+b_2}{a_1+b_1}
\]
which satisfies
\[
    \frac{b_2}{b_1} < \frac{a_2+b_2}{a_1+b_1} < \frac{a_2}{a_1}
.\]
That is, the vector sum must ``lie between'' the two lines which the original vectors are on.
A consequence of this is that, for any pairs of consecutive lines, 
the vector sums of all points along the lines are disjoint.

The remaining claim is that for any choice of points on consecutive lines, the vector sum is distinct.
To show this, consider solutions to
\[
    \lambda_1 v + \lambda_2 w = \lambda_3 v + \lambda_4 w
.\]
We have a solution if and only if
\[
    \left( \lambda_1 - \lambda_3 \right) v + \left( \lambda_2 - \lambda_4 \right) w = 0 
,\]
where \(\lambda_1 \neq \lambda_3\) or \(\lambda_2 \neq \lambda_4\).
This exists only if \(v\) and \(w\) are linearly dependent, which is untrue if
\(\spn{ \left( v \right) } , \spn{ \left( w \right) } \) are distinct lines in \(\mathbb{R} ^{2}\).

We are able to use these facts to prove Theorem \ref{thm:solymosi-sp}.
Begin by applying dyadic partitioning on \(M(A)\) to get
\[
    M(A) = \sum _{x \in \frac{A}{A} } r_{\frac{A}{A} } (x)^{2} \ll  \log \left( \left\lvert \frac{A}{A} \right\rvert   \right) \tau ^{2} \left\lvert S \right\rvert
\]
for some \(\tau\), where \(S = \left\{ x \in \frac{A}{A}  : r_{\frac{A}{A} } (x) \asymp \tau \right\} \).

Consider a reduced system of points and lines, consisting only of the \(\left\lvert S \right\rvert \) many lines which
have \(\asymp \tau\) many points on them. Consider the set of all vector sums between points over
all consecutive lines. Because all pairs of lines give disjoint sets of sums, each with \(\asymp \tau ^{2}\) many distinct sums, there are
\(\tau^{2}\left\lvert S \right\rvert \) many vector sums. The set of all vector sums between points of \(A^{2}\) is a subset of \((A+A)^{2}\),
so
\[
    \tau^{2} \left\lvert S \right\rvert \leq \left\lvert A+A \right\rvert  ^{2}
.\]
Therefore
\[
    \frac{\left\lvert A \right\rvert ^{4}}{\left\lvert AA\right\rvert } \leq M(A) \ll \log \left( \left\lvert \frac{A}{A} \right\rvert   \right) \left\lvert A + A \right\rvert ^{2} \ll \log \left( \left\lvert A \right\rvert  \right)  \left\lvert A+A \right\rvert  ^{2}
\]
so
\[
    \max \left( \left\lvert A + A \right\rvert , \left\lvert AA \right\rvert  \right) \gtrsim \left\lvert A \right\rvert ^{\frac{4}{3} }
.\]

\end{proof}

Many arguments involving energy estimates are not as straightforward
as finding an upper bound. We are often interested in quantites such as 
\(E(A,A + A), E(A - A), E_{3} (A)\) etc.

In the remaining part of this section, we'll prove two theorems concerning these quantities
which we will then apply to prove more advanced results in the next section. Both of these
results come as corollaries of the Szemeredi-Trotter Theorem.

\begin{theorem}
For finite and convex sets \(A \subset \mathbb{R} \),
\[
    E_{3} (A) \lesssim  \left\lvert A \right\rvert ^{ 3}
.\]
\end{theorem}

\begin{proof}
Recall that upon ordering \(a_{i} \) such that \(\delta_A(a_1) \geq \delta_A(a_2) \geq \dots \geq \delta_{A} (a_{\left\lvert A-A \right\rvert } )\),
we have that
\[
    \delta_{A} (a_{r} ) \ll \frac{\left\lvert A \right\rvert }{r^{\frac{1}{3} }}
.\]

With this,
\begin{align*}
E_3(A) & = \sum _{x \in A-A} \delta_{A} (x)^{3} \\
& \ll \left\lvert A \right\rvert ^{3} \sum_{r=1}^{\left\lvert A-A \right\rvert } \frac{1}{r}\\
& \asymp \left\lvert A \right\rvert ^{3} \int_{1}^{\left\lvert A-A \right\rvert } \frac{1}{r}  ~\mathrm{d} r \\
& \asymp  \left\lvert A \right\rvert ^{3} \log \left( \left\lvert A-A \right\rvert  \right) \\
& \lesssim  \left\lvert A \right\rvert ^{3}
\end{align*}
\end{proof}

\begin{theorem} \label{thm:szt-energy-convex-and-arbitrary}
For finite and convex sets \(A \subset \mathbb{R} \), and finite sets \(B \subset \mathbb{R} \)
\[
    E(A,B) \ll \left\lvert A \right\rvert \left\lvert B \right\rvert ^{\frac{3}{2} }
.\]
\end{theorem}

\begin{proof}
Denote the elements of \(A-B\) by \(s_{i} \) where \(\delta_{A,B} (s_1) \geq \cdots \geq \delta_{A,B} (s _{\left\lvert A-B \right\rvert } )\)

Let \(P = \left\{ x \in A - B : \delta_{A,B} (x) \geq \left\lvert B \right\rvert ^{\frac{1}{2} } \right\} \), and let \(P ^{*} = \left( A - B \right) \setminus P\).
\begin{align*}
    \sum _{x \in P} \delta_{A,B} (x)^{2} & =  \sum_{i=1}^{\left\lvert P \right\rvert } \delta_{A,B} (s _{r} )^{2}\\
    & \ll \left\lvert A \right\rvert ^{\frac{2}{3} } \left\lvert B \right\rvert ^{\frac{4}{3} } \sum_{i=1}^{\left\lvert P \right\rvert } \frac{1}{r^{\frac{2}{3} }} \\
    & \asymp \left\lvert A \right\rvert ^{\frac{2}{3} } \left\lvert B \right\rvert ^{\frac{4}{3} } \left\lvert P \right\rvert ^{\frac{1}{3} }\\
    & \ll \left\lvert A \right\rvert ^{\frac{2}{3} } \left\lvert B \right\rvert ^{\frac{4}{3} } \left( \frac{\left\lvert A \right\rvert \left\lvert B \right\rvert ^{2}}{\left\lvert B \right\rvert ^{\frac{1}{2} }}  \right) ^{\frac{1}{3} }\\
    & = \left\lvert A \right\rvert \left\lvert B \right\rvert ^{\frac{3}{2} },
\end{align*}
and
\[
    \sum _{x \in P^{*}} \delta_{A,B} (x)^{2} < \left\lvert B \right\rvert ^{\frac{1}{2} } \sum _{x \in P^{*}} \delta_{A,B} (x) = \left\lvert A \right\rvert \left\lvert B \right\rvert ^{\frac{3}{2} }
.\]

Therefore,
\[
    E(A,B) = \sum _{x \in P} \delta_{A,B} (x)^{2} + \sum _{x \in P ^{*}} \delta_{A,B} (x)^{2} \ll \left\lvert A \right\rvert \left\lvert B \right\rvert ^{\frac{3}{2} } 
.\]
\end{proof}

\section{ Application of Energy Estimates}
This section will employ results from the previous section to prove Theorems \ref{thm:shkredov-diff}, \ref{thm:shkredov-sum}.
This argument can be found in \cite{shkredov}.

Throughout this argument, in the same convention as \cite{shkredov}, we'll use the
notation
\[
    A_{x} = A \cap \left( A + x \right) = \left\{ a \in A : a - x \in A \right\} = \left\{ a \in A : \exists a_0 \in A ~~\text{s.t.}~~ a - a_0 = x \right\} 
.\]
Observe that
\[
    \left\lvert A_{x}  \right\rvert = \delta_{A} (x) 
.\]


We require the following lemmata, proven in \cite{Schoen-lemma} and \cite{shkredov}.
\begin{lemma}
For every set \(A \subset \mathbb{R} \) we have
\[
    \sum _{x} E\left( A, A_{x}  \right) = E_{3} (A)
.\]
\end{lemma}

\begin{proof}
For a set \(S\), let the function \(S\) be the indicator function, so
\[S(x)= 
\begin{cases}
1 \text{ if } x \in S\\
0 \text{ otherwise}
\end{cases}
\]

We begin by observing that
\[
    A_{x} (s) A_{x} (s + t) = A(s) A(s + x) A(s + t) A(s+t+x) = A_{t} (s) A_{t} (s + x)
,\]
and therefore that
\[
    \delta_{A_{x} } (t) = \sum _{s} A_{x} (s) A_{x} (s + t) = \sum _{s} A_{t} (s) A_{t} (s + x) = \delta_{A_{t} } (x)
.\]

With this, we have
\begin{align*}
    \sum _{x} E(A,A_{x} )& = \sum _{s} \sum _{x} \delta_{A} (s) \delta_{A_{x} } (s)\\
    & = \sum _{s} \delta_{A} (s) \left( \sum _{x} \delta_{A_{x} } (s) \right) \\
    & = \sum _{s} \delta_{A} (s) \left( \sum _{x}  \delta_{A_{s} } (x) \right) \\
    & = \sum _{s} \delta_{A} (s) \left\lvert A_{s}  \right\rvert ^{2}\\
    & = E_{3} (A)
\end{align*}

\end{proof}

\begin{lemma}\label{lem:shkredov-main-lem}
For every set \(A \subset \mathbb{R} \), and \(P \subset A - A\), if \(\eta\) is the number for which
\[
    \sum _{x \in P} \left\lvert A_{x}  \right\rvert = \eta \left\lvert A \right\rvert ^{2}
,\]
then
\[
    \sum _{x \in P} \left\lvert A \pm A_{x}  \right\rvert \geq \frac{\eta ^{2} \left\lvert A \right\rvert ^{6}}{E_{3} (A)} 
.\]
\end{lemma}

\begin{proof}
\begin{align*}
\left\lvert A \right\rvert \left\lvert A_{x}  \right\rvert & = \sum _{s \in A + A_{x} } \sigma_{A, A_{x} } (s) = \sum _{s \in A-A} \delta_{A, A_{x} } (s)\\
& \leq E\left( A, A_{x}  \right) ^{\frac{1}{2} } \left\lvert A \pm  A_{x}  \right\rvert ^{\frac{1}{2} }
\end{align*}
so
\begin{align*}
\eta \left\lvert A \right\rvert ^{3} & =  \sum _{x \in P} \left\lvert A \right\rvert \left\lvert A_{x}  \right\rvert \\
& \leq \sum _{x \in P} E(A, A_{x} )^{\frac{1}{2} }\left\lvert A \pm A_{x}  \right\rvert ^{\frac{1}{2} } \\
& \leq \left( \sum _{x\in P} E(A,A_{x} ) \right)  ^{\frac{1}{2} } \left( \sum _{x \in P} \left\lvert A \pm A_{x}  \right\rvert  \right) ^{\frac{1}{2} }
\end{align*}
and therefore
\[
    \sum _{x \in P} \left\lvert A \pm A_{x}  \right\rvert \geq \frac{\eta ^{2}\left\lvert A \right\rvert ^{6}}{E_{3} (A)} 
.\]

\end{proof}

We're now able to prove the theorems. Recall the following theorems

\begin{theorem*}
For finite and convex sets \(A \subset \mathbb{R} \),
\[
    \left\lvert A + A \right\rvert \gtrsim  \left\lvert A \right\rvert ^{\frac{20}{13}  }
.\]
\end{theorem*}
and
\begin{theorem*}
For finite and convex sets \(A \subset \mathbb{R} \),
\[
    \left\lvert A - A \right\rvert \gtrsim \left\lvert A \right\rvert ^{\frac{8}{5} }
.\]
\end{theorem*}

\begin{proof}[Proof of Theorems \ref{thm:shkredov-diff}, \ref{thm:shkredov-sum}]
Denote the difference and sum sets by \(D = \left\lvert A-A \right\rvert \) and \(S = \left\lvert A+A \right\rvert \).
Observe that
\[
    A - A_{x} \subset D \cap D_{x} 
\]
and
\[
    A + A_{x} \subset S \cap S_{x} 
.\]
We use this observation along with Lemma \ref{lem:shkredov-main-lem} to find bounds on
the additive energies between \(D\) and \(S\) and \(A\).

Consider the popular sets of differences \(P,P'\) defined by
\[
    P = \left\{ x \in A - A : \delta_{A} (x) \geq \frac{\left\lvert A \right\rvert ^{2}}{2 \left\lvert A-A \right\rvert }  \right\} 
\]
and
\[
    P ' = \left\{ x \in A-A : \delta_{A} (x) \geq \frac{\left\lvert A \right\rvert ^{2}}{2\left\lvert A + A \right\rvert }  \right\} 
.\]

Let \(P^{*}\) and \(P^{'*}\) be their respective compliments. We have
\[
    \sum _{x \in P^{*}} \left\lvert A_{x} \right\rvert < \frac{\left\lvert A \right\rvert ^{2}}{2 \left\lvert A-A \right\rvert } \cdot  \left\lvert P^{*} \right\rvert \leq \frac{\left\lvert A \right\rvert ^{2}}{2} 
,\]
so
\begin{equation} \label{eq:shkredov-1}
    \sum _{x \in P} \left\lvert A_{x}  \right\rvert \gg \left\lvert A \right\rvert ^{2}
    .
\end{equation}

We also have
\[
    \sum _{x \in P ^{'*}} \left\lvert A_{x}  \right\rvert ^{2} < \frac{\left\lvert A \right\rvert ^{2}}{2 \left\lvert A+A \right\rvert } \cdot \left\lvert A \right\rvert ^{2}
,\]
so
\[
    \sum _{x \in P'} \left\lvert A_{x}  \right\rvert ^{2} \gg \frac{\left\lvert A \right\rvert ^{4}}{\left\lvert A+A \right\rvert } 
.\]

We can directly apply (\ref{eq:shkredov-1}) to Lemma \ref{lem:shkredov-main-lem}. We have
\[
    \eta \left\lvert A \right\rvert ^{2} = \sum _{x \in P} \left\lvert A_{x}  \right\rvert \gg \left\lvert A \right\rvert ^{2} \implies \eta \gg 1
\]
so
\[
    \sum _{x \in P} \left\lvert A \pm  A_{x}  \right\rvert \gg \frac{\left\lvert A \right\rvert ^{6}}{E_{3} (A)} \gtrsim \left\lvert A \right\rvert ^{3}
.\]

Recall that \(A - A_{x} \subset D \cap D_{x} \), or \(\left\lvert A - A_{x}  \right\rvert \leq \left\lvert D_{x}  \right\rvert = \delta_{D} (x)\). It follows that
\[
    \sum _{x \in P} \delta_{D} (x) \gtrsim \left\lvert A \right\rvert ^{3}
.\]
By the definition of \(P\) it follows that
\[
    E(A,D) \geq \sum _{x \in P} \delta_{A} (x)\delta_{D} (x) \geq \frac{\left\lvert A \right\rvert ^{2}}{2 \left\lvert A-A \right\rvert } \sum _{x \in P} \delta_{D} (x) \gtrsim \frac{\left\lvert A \right\rvert ^{5}}{\left\lvert A-A \right\rvert }
.\]
By applying Theorem \ref{thm:szt-energy-convex-and-arbitrary},
\[
    \frac{\left\lvert A \right\rvert ^{5}}{\left\lvert A-A \right\rvert } \lesssim \left\lvert A \right\rvert \left\lvert A-A \right\rvert ^{\frac{3}{2} } \implies \left\lvert A-A \right\rvert \gtrsim \left\lvert A \right\rvert ^{\frac{8}{5} }
.\]

By applying \ref{shkredov-lemma-szt}, we have that
\[
    \frac{\left\lvert A \right\rvert ^{4}}{\left\lvert A+A \right\rvert } \ll \sum _{x \in P'} \left\lvert A_{x}  \right\rvert ^{2} \ll \sum _{x \in P' : \left\lvert A_{x}  \right\rvert \ll L} \left\lvert A_{x}  \right\rvert ^{2}
.\]

We apply a dyadic partitioning to get
\begin{align*}
\frac{\left\lvert A \right\rvert ^{4}}{\left\lvert A+A \right\rvert } &  \ll \sum _{x \in P' : \left\lvert A_{x}  \right\rvert \ll \frac{\left\lvert A+A \right\rvert }{\left\lvert A \right\rvert } } \left\lvert A_{x}  \right\rvert ^{2}\\
& \lesssim \Delta \sum _{x \in D} \left\lvert A_{x}  \right\rvert 
\end{align*}
where
\[
    D = \left\{ x \in P' : \frac{\left\lvert A+A \right\rvert }{\left\lvert A \right\rvert }  \gg \left\lvert A_{x}  \right\rvert \asymp \Delta \right\} 
.\]
This gives
\[
    \sum _{x \in D} \left\lvert A_{x}  \right\rvert \gtrsim \frac{\left\lvert A \right\rvert ^{5}}{\left\lvert A+A \right\rvert ^{2}} 
.\]

Applying Lemma \ref{lem:shkredov-main-lem}, we get
\[
    \sum _{x \in D} \left\lvert A + A_{x}  \right\rvert \gtrsim \frac{\left\lvert A \right\rvert ^{9}}{\left\lvert A+A \right\rvert ^{4}} 
.\]

Recalling that \(A + A_{x} \subset S_{x} \), we have
\[
    \sum _{x \in D} \delta_{S} (x) \geq \sum _{x \in D} \left\lvert A + A_{x}  \right\rvert \gtrsim \frac{\left\lvert A \right\rvert ^{9}}{\left\lvert A+A \right\rvert ^{4}} 
,\]
which, by the definition of \(P'\) gives
\[
    \frac{\left\lvert A \right\rvert ^{11}}{\left\lvert A+A \right\rvert ^{5}} \lesssim \sum _{x \in D} \delta_{S} (x) \delta_{A} (x) \leq E(A,S) \ll \left\lvert A \right\rvert \left\lvert A+A \right\rvert ^{\frac{3}{2} }
,\]
or
\[
    \left\lvert A+A \right\rvert \gtrsim \left\lvert A \right\rvert ^{\frac{20}{13} }
.\]
\end{proof}
%────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
%────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────

\subsection*{Acknowledgements}
This report is based on work supported by NSF grant DMS-2051032. I'd like to express my gratitude to the Department of Mathematics at Indiana University Bloomington, as well as thanks to my advisor Shukun Wu, who greatly assisted me in my studies.


%% TIP: Only entries which you actually cite with \cite somewhere else in the .tex will appear.
\bibliographystyle{amsalpha}
\bibliography{bibliography} % the file "bibliography.bib" will be needed

\section*{Appendix}

\begin{theorem*}[Uniqueness of Binary Representations]
For any \(x \in \mathbb{N}_{0}  \), there is a unique sequence of numbers \(\left( a_{i}  \right) _{i = 0} ^{I} \), with
\(I \leq \log _{2} \left( x \right) \) and \(a_{i} \in \left\{ 0,1 \right\} \) such that
\[
    x = \sum _{i \leq \log _{2} \left( x \right) } a_{i} 2^{i}
\]
\end{theorem*}

\begin{proof}
Let \(x \in \mathbb{N}_{0}  \) and suppose that there are two binary representations for \(x\),
that is, there are \(r,s \in \mathbb{N}_{0}  \), and sequences \((a_{i} )_{i = 0} ^{r-1}\) and \(\left( b_{i}  \right) _{i = 0} ^{s - 1}\)
such that
\[
    x = 2^{r} + a_{r-1}2^{r-1} + \cdots + a_{0} = 2^{s} + b_{s - 1} 2^{s - 1} + \cdots + b_0  
.\]

We first prove that both binary representations must be of the same degree, or have
the same highest power of 2. Without loss of generality, suppose \(s > r\). It follows that
\[
    x = 2^{r} + a_{r-1} 2^{r-1} + \cdots + a_0 \leq \sum _{i \leq r} 2^{i} = 2^{r+1} - 1 < 2^{s} \leq 2^{s} + b_{s-1} 2^{s-1} + \cdots  + b_0
,\]
a contradiction. With the fact that any representations must be of the same degree,
we have that
\[
    0 = \left( a_{r-1} - b_{r-1}  \right) 2^{r-1} + \cdots + \left( a_0-b_0 \right)
\]
is of degree 0, and therefore that for all \(i\), \(a_{i} = b_{i} \), or that
the binary representation is unique.

\end{proof}

Past this point in the appendix, we will freely use shorthand which was discussed in the preliminaries
section. We also introduce the notation
\[
    f(x) = o(g(x))
\]
if
\[
    \lim_{x \to \infty} \frac{f(x)}{g(x)} = 0 
.\]

We'll introduce the following theorems together due to their relatedness.

\begin{theorem*}
\[
    \left\lvert [n] \cdot [n] \right\rvert \gg \frac{n ^{2}}{\log \left( n \right) }    
.\]
\end{theorem*}

\begin{theorem*}
\[
    \left\lvert [n] \cdot [n] \right\rvert = o(n^{2})
.\]
\end{theorem*}

This is known as the ``multiplication table theorem'' because the quantity
\[
    \left\lvert [n] \cdot [n] \right\rvert 
\]
is the number of distinct numbers in an \(n \times n\) multiplication table:

\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\textwidth]{mult-table.png}
    \caption{10 \(\times \) 10 multiplication table with distinct numbers highlighted in red.}
\end{figure}

These theorems are just bounds on the asymptotic behavior of \(\left\lvert [n] \cdot [n] \right\rvert \). 
In \cite{Ford}, it is proven that the exact order of this quantity is
\[
    \left\lvert [n] \cdot [n] \right\rvert \asymp \frac{n^{2}}{\log \left( n \right) ^{\delta} \left( \log \log \left( n \right) \right) ^{\frac{3}{2} } } 
,\]
where
\[
    \delta = 1 - \frac{1 + \log \log \left( 2 \right)  }{2} 
.\]
The exact asymptotic behavior of this quantity is not known.
To prove the first, we require a lemma and a prerequisite theorem.

\begin{lemma*}[Abel's Summation Formula]
Let \(\left( a_{n}  \right) _{n = 1} ^{\infty }\) be a sequence of real numbers. Let \(A: \mathbb{R} \to \mathbb{R} \) be defined by
\[
    A(t) = \sum _{n \leq t} a_{n} 
.\]
For \(x \in \mathbb{R} \) and any differentiable function \(\phi : [1,y] \to \mathbb{R} \),
\[
    \sum _{n \leq x} a_{n} \phi(n) = A(x)\phi(x) - \int_{1}^{x} A(t)\phi'(t) ~\mathrm{d} t
\]
\end{lemma*}

\begin{proof}
\begin{align*}
    \sum _{n \leq x} a_{n} \phi(n) & = a_1\phi(1 ) + \cdots  + a_{\left\lfloor x \right\rfloor } \phi(\left\lfloor x \right\rfloor )\\
    & = A(1) \phi(1)+ \left( A(2) - A(1)\right)\phi(2)  + \cdots + \left( A(\left\lfloor x \right\rfloor ) - A(\left\lfloor x \right\rfloor -1) \right) \phi(\left\lfloor x \right\rfloor )\\
    & = \left( \phi(1) - \phi(2) \right) A(1) + \cdots + \left( \phi(\left\lfloor x \right\rfloor -1)- \phi(\left\lfloor x \right\rfloor ) \right) A \left( \left\lfloor x \right\rfloor -1 \right) + \phi(\left\lfloor x \right\rfloor )A(\left\lfloor x \right\rfloor )\\
    & = \phi(\left\lfloor x \right\rfloor )A(\left\lfloor x \right\rfloor ) - \sum _{i = 2} ^{\left\lfloor x \right\rfloor } \left( \phi(i) - \phi(i-1) \right) A(i - 1)\\
    & = \phi\left( \left\lfloor x \right\rfloor  \right) A (\left\lfloor x \right\rfloor ) - \sum_{i=2}^{\left\lfloor x \right\rfloor } \left( \int_{i-1}^{i} \phi'(t) ~\mathrm{d} t  \right) A(i-1)
\end{align*}

Observe that, for \(i\in \mathbb{Z} \) and \(t \in [i - 1,i)\),
\[
    A(t) = A(\left\lfloor t \right\rfloor ) = A(i - 1)
,\]
so

\begin{align*}
    \sum _{n \leq x} a_{n} \phi(n) & = \phi\left( \left\lfloor x \right\rfloor  \right) A (\left\lfloor x \right\rfloor ) - \sum_{i=2}^{\left\lfloor x \right\rfloor } \left( \int_{i-1}^{i} \phi'(t) ~\mathrm{d} t  \right) A(i-1)\\
    & = \phi(\left\lfloor x \right\rfloor ) A(\left\lfloor x \right\rfloor ) - \sum_{i=2}^{\left\lfloor x \right\rfloor } \int_{i-1}^{i} A(t)\phi'(t) ~\mathrm{d} t\\
    & = \phi(\left\lfloor x \right\rfloor )A(\left\lfloor x \right\rfloor )- \int_{1}^{\left\lfloor x \right\rfloor } A(t)\phi'(t) ~\mathrm{d} t
\end{align*}
and
\begin{align*}
    \int_{1}^{\left\lfloor x \right\rfloor } A(t)\phi'(t) ~\mathrm{d} t & = \int_{1}^{x} A(t)\phi'(t) ~\mathrm{d} t - \int_{\left\lfloor x \right\rfloor }^{x} A(t)\phi'(t) ~\mathrm{d} t \\
    & = \int_{1}^{x} A(t)\phi'(t) ~\mathrm{d} t -  A(\left\lfloor x \right\rfloor )\left(\phi(x)- \phi(\left\lfloor x \right\rfloor ) \right) \\
    & =\int_{1}^{x} A(t)\phi'(t) ~\mathrm{d} t - A(x)\phi(x) + A(\left\lfloor x \right\rfloor )\phi(\left\lfloor x \right\rfloor ).
\end{align*}
Substituting this we get
\begin{align*}
    \sum _{n \leq x} a_{n} \phi(n) & = \phi(\left\lfloor x \right\rfloor )A(\left\lfloor x \right\rfloor )- \int_{1}^{\left\lfloor x \right\rfloor } A(t)\phi'(t) ~\mathrm{d} t\\
    & = A(x) \phi(x) - \int_{1}^{x} A(t)\phi'(t) ~\mathrm{d} t
\end{align*}
\end{proof}

The following prerequisite theorem gives the order of magnitude of 3 functions which
will prove useful.

\begin{theorem}
Define the functions \(\theta,\psi, \pi : [1, \infty) \to \mathbb{R} \) by
\[
    \theta(x) = \sum _{\substack{ p \leq x \\ p \text{ prime}  }} \log \left( p \right) 
,\]
\[
    \psi(x) = \sum _{\substack{ p^{\alpha} \leq x \\ p \text{ prime} \\ \alpha \in \mathbb{N}  }} \log \left( p \right) 
,\]
\[
    \pi(x) = \sum _{\substack{ p \leq x \\ p \text{ prime}  }} 1
.\]
We have
\[
    \psi(x) \asymp \theta(x) \asymp x
\]
and
\[
    \pi(x) \asymp \frac{x}{\log \left( x \right) } 
.\]
\end{theorem}

\begin{proof}
The functions \(\theta\) and \(\psi\) are clearly related by
\[
    \psi(t) = \prod _{\substack{ p \leq N^{\frac{1}{\alpha} } \\ p \text{ prime} \\ \alpha \in \mathbb{N}  }} \log \left( p \right) = \sum _{\alpha \in \mathbb{N} } \theta\left( t ^{\frac{1}{\alpha} } \right)  = \theta(t) + \sum _{\alpha \geq 2} \theta \left( t ^{\frac{1}{\alpha} } \right) 
.\]
Note that the sum over \(\alpha\) has only finitely many terms. The sum terminates when
\[
    2 \geq  t^{\frac{1}{\alpha} } \implies  \alpha \leq \log _{2} \left( t \right) 
.\]

A trivial upper bound on \(\theta (t)\) is
\[
    \theta(t) = \sum _{\substack{ p \leq t \\ p \text{ prime}}} \log \left( p \right) \leq t \log \left( t \right)  
,\]
so
\begin{align*}
    \psi(t) & = \theta(t) + \sum _{2 \leq \alpha \leq \log _{2} \left( t \right) } \theta(t ^{\frac{1}{\alpha} }) \\
    & \leq \theta(t) + \log _{2} \left( t \right) \theta(t^{\frac{1}{2} }) \\
    & \leq \theta(t) + \frac{t^{\frac{1}{2} }\log \left( t \right) ^{2}}{\log \left( 2 \right) }
\end{align*}
or
\[
    \psi(t) \ll \max \left( \theta(t), t^{\frac{1}{2} }\log \left( t \right) ^{2} \right) 
.\]
By showing \(\theta(t) \ll t\), we will have shown \(\psi(t) \ll \theta(t) \ll t\). Because \(\theta(t) \leq \psi(t)\),
we will have shown that \(\psi(t) \asymp \theta(t) \ll t\).

We have
\[
    \theta(t) = \sum _{\substack{ p \leq t \\ p \text{ prime}  }} \log \left( p \right) = \log \left( \prod _{\substack{ p \leq t \\  p \text{ prime}  }} p \right) 
,\]
so it is sufficient to show that
\[
    \prod _{\substack{ p \leq t \\ p \text{ prime}  }} p \ll e^{t}
.\]
It is also sufficient to prove it for \(t \in \mathbb{N} \) because \(\theta(t) = \theta\left( \left\lfloor t \right\rfloor  \right) \).

For some natural number \(t\), and a prime \(p\),
\[
    t + 1 < p \leq 2t + 1 \implies p \mid \binom{2t + 1}{t} = \frac{\left( 2t + 1 \right) !}{t! (t+1)!} 
.\]
Therefore, for any \(t\),
\[
    \prod _{\substack{ t + 1 < p \leq 2t + 1\\ p \text{ prime}   }} p   \mid \binom{2t+1}{t} \implies \prod _{\substack{ t + 1 < p \leq 2t + 1\\ p \text{ prime}   }} p   \leq  \binom{2t+1}{t}
,\]
which gives us
\[
    2 \prod _{\substack{ t + 1 < p \leq 2t + 1\\ p \text{ prime}   }} p   \leq  2 \binom{2t+1}{t} \leq (1+1)^{2t + 1} \implies \prod _{\substack{ t + 1 < p \leq 2t + 1\\ p \text{ prime}   }} p  \leq 4^{t}
.\]

The rest follows by induction on \(t\). Because we are proving
a statement about order of magnitude, the base case is trivial. Now suppose that
for some \(t \in \mathbb{N} \),
\[
    \prod _{\substack{ p \leq m \\p \text{ prime}  }} p \ll e^{t}
.\]
If \(t\) is odd, the induction follows trivially. If \(t\) is even, let \(t = 2m\),
so
\[
    \prod  _{\substack{ p \leq 2m \\ p \text{ prime}  }} p\ll e^{2m}
.\]
We have
\begin{align*}
\prod _{\substack{ p \leq 2m + 1 \\ p \text{ prime}  }} p & = \prod _{\substack{ p \leq m + 1 \\ p \text{ prime}  }} p \prod _{\substack{ m + 1 < p \leq 2m + 1 \\ p \text{ prime}  }} p \\
& \ll e^{m + 1} 4^{m} \\
& \ll e^{m+1} e^{m} = e^{2m+1}. 
\end{align*}
Thus, \(\psi(t) \asymp \theta(t) \ll t\). To prove \(\psi(t) \asymp \theta(t) \asymp t\),
it suffices to show that \(\psi(t) \gg t\).

Observe that for some number \(N \in \mathbb{N} \), the prime factorization of \(N!\) is of the form
\[
    N! = \prod _{\substack{ p \leq N \\ p \text{ prime}  }} p ^{\alpha(N,p)}
,\]
where
\[
    \alpha(N,p) = \sum _{i \in \mathbb{N} } \left\lfloor \frac{N}{p ^{i}}  \right\rfloor = \sum _{i \leq \log _{p} \left( N \right) } \left\lfloor \frac{N}{p ^{i}}  \right\rfloor  =\sum _{i \leq \log _{2} \left( N \right) } \left\lfloor \frac{N}{p ^{i}}  \right\rfloor 
.\]

Therefore,
\[
    \binom{2n}{n} = \frac{\left( 2n \right) !}{\left( n!  \right) ^{2}} = \prod _{\substack{ p \leq 2n \\ p \text{ prime}  }} p ^{ \sum _{i \in \mathbb{N} } \left( \left\lfloor \frac{2n}{p^{i}}  \right\rfloor - 2 \left\lfloor \frac{n}{p ^{i}}  \right\rfloor \right)  }
.\]

In general for some \(x \in \mathbb{R}, k \in \mathbb{N} \), if \(k \leq x < k +1\), then
\[
    2k \leq 2x < 2k + 2
\]
so
\[
    2\left\lfloor x \right\rfloor \leq \left\lfloor 2x \right\rfloor \leq 2\left\lfloor x \right\rfloor +1
.\]

It follows that
\[
    \left\lfloor \frac{2n}{p^{i}}  \right\rfloor - 2 \left\lfloor \frac{n}{p ^{i}}  \right\rfloor \leq  1
\]
so
\[
    \sum _{i \in \mathbb{N} } \left( \left\lfloor \frac{2n}{p^{i}}  \right\rfloor - 2 \left\lfloor \frac{n}{p ^{i}}  \right\rfloor  \right) \leq \sum_{i \leq \log _{p} \left( 2n \right) } 1 = \left\lfloor \frac{\log \left( 2n \right) }{\log \left( p \right) }  \right\rfloor 
,\]
and therefore
\[
    \log \binom{2n}{n} \leq \sum _{\substack{ p \leq 2n \\ p \text{ prime}  }} \left\lfloor \frac{\log \left( 2n \right) }{\log \left( p \right) }  \right\rfloor \log \left( p \right) = \psi(2n)
.\]

Observing that
\[
    \log \binom{2n}{n} = \log \left( \frac{n+ 1}{1} \cdot \frac{n + 2}{2} \cdot \cdots \cdot \frac{n + n }{n}   \right) \geq \log \left( 2^{n} \right) \gg n
,\]
it follows that
\[
    \psi(t) \gg t
.\]

We have shown
\[
    \psi(t) \asymp \theta(t) \asymp t
.\]

By applying Abel's summation formula,
\begin{align*}
\pi(x) & = \sum _{\substack{ p \leq x \\ p \text{ prime}  }} 1\\
& = \sum _{\substack{ p \leq x \\ p \text{ prime}  }}  \log \left( p \right) \cdot \frac{1}{\log \left( p \right) } \\
& = \theta(x)\cdot \frac{1}{\log \left( x \right) } + \int_{2}^{x} \frac{\theta(t)}{t \log ^{2}\left( t \right) }  ~\mathrm{d} t\\
& \asymp \frac{x}{\log \left( x \right) } + \int_{2}^{x} \frac{1}{\log ^{2}\left( t \right) }  ~\mathrm{d} t 
\end{align*}
and
\begin{align*}
    \frac{x}{\log \left( x \right) } & \asymp \int_{2}^{x} \left( \frac{t}{\log \left( t \right) } \right) '  ~\mathrm{d} t \\
    & = \int_{2}^{x} \frac{1}{\log \left( t \right) }  ~\mathrm{d} t - \int_{2}^{x} \frac{1}{\log ^{2}\left( t \right) }  ~\mathrm{d} t \\
    & \gg \int_{2}^{x} \frac{1}{\log ^{2}\left( t \right) }  ~\mathrm{d} t 
\end{align*}
so
\[
    \pi(x) \asymp \frac{x}{\log \left( x \right) } 
.\]
\end{proof}

We are now able to prove the first theorem.

\begin{theorem*}
    \[
        \left\lvert [n] \cdot [n] \right\rvert \gg \frac{n ^{2}}{\log \left( n \right) }    
    .\]
\end{theorem*}

\begin{proof}
Let \(p_{i} \) be the \(i\)-th prime number and \(k\) be chosen such that \(p_{k} \) is the largest prime
\(p_{k} \leq n\). Consider the set of numbers
\[
    P = \left\{ p_{i} \cdot m : i \leq k ~,~ m \leq p_{i}  \right\} 
.\]
We have \(P \subset [n] \cdot [n]\) and \(p_{i} \cdot m\) is distinct for every choice of \(i,m\).

It follows that, by applying Abel's summation formula,
\begin{align*}
\left\lvert [n]\cdot [n] \right\rvert & \geq \left\lvert P \right\rvert \\
& = \sum _{\substack{ p \leq n \\ p \text{ prime}  }} p\\
& = \pi(n)\cdot n - \int_{2}^{n} \pi(t) ~\mathrm{d} t \\
& \gg \pi(n)\cdot n\\
& \gg \frac{n^{2}}{\log \left( n \right) } 
\end{align*}
\end{proof}

We require 2 additional lemmata to prove the second theorem.

\begin{lemma*}
Let \(X\) be a real random variable with variance \(\sigma^{2}\). For any \(t \in \mathbb{R} ^{+}\),
\[
    \mathbb{P} \left( \left\lvert X - \mathbb{E} (X) \right\rvert \geq t \right) \leq \frac{\sigma^{2}}{t^{2}} 
\]
\end{lemma*}

\begin{proof}
We prove this for the case of discrete \(X\).
The proof for continuous \(X\) follows similarly.

Let \(N\) be the number of possible values of \(X\).

For any interval \(I \subset \mathbb{R} \), let \(1_{I} : \mathbb{R} \to \left\{ 1,0 \right\} \) be
defined by
\[
    1_{I} (x) = 1 \text{ if } x \in I ~,~ 1_{I} (x) = 0 \text{ if } x \not \in I
.\]

We have
\[
    \mathbb{P} (X \geq t) = \frac{ \sum _{x} 1_{[t,\infty )} (x)}{N} \leq \frac{ \sum _{x} \frac{x}{t} }{N} = \frac{ \sum _{x} \frac{x}{N} }{t}  = \frac{\mathbb{E} (X)}{t} 
\]
so
\[
    \mathbb{P} \left( \left\lvert X - \mathbb{E} (x) \right\rvert \geq t \right) =\mathbb{P} \left( \left\lvert X - \mathbb{E} (X) \right\rvert ^{2} \geq t ^{2} \right) \leq \frac{\sigma^{2}}{t^{2}} 
.\]
\end{proof}

\begin{lemma*}
\[
    \sum _{\substack{ p \leq n \\ p\text{ prime}  }} \frac{1}{p}  = \log\log \left( n \right) + O(1)
.\]
\end{lemma*}

\begin{proof}
Recall that for some number \(N \in \mathbb{N} \), the prime factorization of \(N!\) is of the form
\[
    N! = \prod _{\substack{ p \leq N \\ p \text{ prime}  }} p ^{\alpha(N,p)}
,\]
where
\[
    \alpha(N,p) = \sum _{i \in \mathbb{N} } \left\lfloor \frac{N}{p ^{i}}  \right\rfloor = \sum _{i \leq \log _{p} \left( N \right) } \left\lfloor \frac{N}{p ^{i}}  \right\rfloor  =\sum _{i \leq \log _{2} \left( N \right) } \left\lfloor \frac{N}{p ^{i}}  \right\rfloor 
.\]
It follows that
\begin{align*}
    \log \left( N! \right) & = \sum _{\substack{ p \leq N \\p \text{ prime}  }} \alpha(N,p) \log \left( p \right) \\
    & = \sum _{\substack{ p \leq N \\p \text{ prime} \\ i \leq \log _{p} \left( N \right)   }} \left\lfloor \frac{N}{p ^{i}}  \right\rfloor \log \left( p \right) \\
    & = \sum _{\substack{ p \leq N \\ p \text{ prime} \\ i \leq \log _{p} \left( N \right)  }} \left( \frac{N}{p ^{i}} - \delta(p) \right) \log \left( p \right) \\
    & = N\sum _{\substack{ p \leq N \\ p \text{ prime} \\ i \leq \log _{p} \left( N \right) }} \frac{\log \left( p \right) }{p ^{i}} - \sum _{\substack{ p \leq N \\ p \text{ prime} \\ i \leq \log _{p} \left( N \right)  }} \delta(p) \log \left( p \right) .
\end{align*}
We have that
\[
    i \leq \log _{p} \left( N \right) \iff p ^{i} \leq N
,\]
so
\[
    \sum  _{\substack{ p \leq N \\ p \text{ prime} \\ i \leq \log _{p} \left( N \right)  }} \log \left( p \right) = \psi(N)
,\]
and therefore
\[
    \sum _{\substack{ p \leq N \\ p \text{ prime} \\ i \leq \log _{2} \left( N \right)  }} \frac{\log \left( p \right) }{p ^{i}} \leq  \frac{\log \left( N! \right) }{N} + \frac{\psi(N)}{N} 
.\]

We also have, via a Riemann sum,
\begin{align*}
    \log \left( N! \right) & = \sum_{i=1}^{N} \log \left( i \right) \\
    & = \int_{2}^{N} \log \left( i \right)  ~\mathrm{d} i + O(1) \\
    & = \left. \left[ i\log \left( i \right) - i \right]  \right|_{i = 1}^{N} + O(1)\\
    & = N \log \left( N \right) - N + O(1).
\end{align*}

This leads to
\[
    \sum _{\substack{ p \leq N \\ p \text{ prime} \\ i \leq \log _{2} \left( N \right) }} \frac{\log \left( p \right) }{p ^{i}} = \frac{N \log \left( N \right) - N + O(1)}{N} + \frac{O(N)}{N} = \log \left( N \right)  + O(1)
.\]

Moreover, we have that
\begin{align*}
\sum  _{\substack{ p \leq N \\ p \text{ prime} \\ 2 \leq i \leq \log _{2} \left( N \right)  }} \frac{\log \left( p \right) }{p ^{i}} & \leq \sum  _{\substack{ x,i \geq 2}} \frac{\log \left( x \right) }{x ^{i}} \\
& = \sum _{x \geq 2} \sum _{i \geq 2} \frac{\log \left( x \right) }{x^{i}} \\
& = \sum _{x \geq 2} \frac{1}{x^{2}} \left( \frac{\log \left( x \right) }{1-\frac{1}{x} }  \right)\\
& = \sum _{x \geq 2} \frac{\log \left( x \right) }{x ^{2} - x} 
\end{align*}
which, by the limit comparison test, converges if
\[
    \sum _{x \geq 2} \frac{\log \left( x \right) }{x^{2}}
\]
converges.

Apply L'Hopital's rule to see that
\[
    \forall \epsilon > 0 ~,~ \log \left( x \right) = o(x^{\epsilon})
,\]
and therefore that
\begin{align*}
\sum _{x \geq 2} \frac{\log \left( x \right)}{x ^{2}} & = \sum _{x\geq 2} \frac{o(1)}{x^{2 - \epsilon}} \\
& \ll \sum _{x \geq 2} \frac{1}{x^{2-\epsilon}} \\
& \asymp \int_{2}^{\infty} \frac{1}{x^{2-\epsilon}}  ~\mathrm{d} x 
\end{align*}
which converges for \(\epsilon < 1\). It follows that
\[
    \sum _{\substack{ p \leq N \\p \text{ prime}  }}\frac{ \log \left( p \right) }{p} = \log \left( N \right) + O(1)
.\]

Applying Abel's summation formula,
\begin{align*}
\sum  _{\substack{ p \leq N \\p \text{ prime}  }} \frac{1}{p} & = \sum _{\substack{ p \leq N \\ p \text{ prime}  }}   \frac{\log \left( p \right) }{p} \cdot \frac{1}{\log \left( p \right) } \\
& = \left( \sum _{\substack{ p \leq N \\ p \text{ prime}  }} \frac{\log \left( p \right) }{p}  \right) \cdot \frac{1}{\log \left( N \right) } + O(1) + \int_{2}^{N} \frac{ \sum _{\substack{ p \leq t \\ p \text{ prime}  }}\frac{ \log \left( p \right) }{p} }{t \log ^{2}\left( t \right) }  ~\mathrm{d} t \\
& = \frac{ \log \left( N \right) + O(1)  }{\log \left( N \right) } + O(1) + \int_{2}^{N} \frac{\log \left( t \right) + O(1)}{t \log ^{2}\left( t \right) }  ~\mathrm{d} t\\
& = O(1) + \int_{2}^{N} \frac{1}{t \log \left( t \right) }  ~\mathrm{d} t + \int_{2}^{N} \frac{O(1)}{t \log ^{2}\left( t \right) }  ~\mathrm{d} t \\
& = \log\log \left( N \right) + O(1)
\end{align*}
\end{proof}

We are now able to prove

\begin{theorem*}
    \[
        \left\lvert [n] \cdot [n] \right\rvert = o(n^{2})
    .\]
\end{theorem*}

\begin{proof}
Define the prime power counting function \(\Omega : \mathbb{N} \to \mathbb{N} \) by
\[
    \Omega(x) = \sum _{\substack{ p ^{\alpha} \mid x \\ p \text{ prime} \\ \alpha \in \mathbb{N}  }} 1
.\]

Let \(N \in \mathbb{N} \). We prove this theorem by examining the distribution of \(\Omega(X)\) for an
uniformly distributed random variable \(X\) of natural numbers \(x \leq N\).
Let \(Y = \Omega(X)\).

We first calculate \(\mathbb{E} (Y)\).

\begin{align*}
\mathbb{E} (Y) & = \sum _{x \leq N} \frac{1}{N} \cdot \Omega(x)\\
& = \frac{1}{N} \sum _{x \leq N } \sum _{\substack{ p^{\alpha} \mid x \\ p \text{ prime} \\ \alpha \in \mathbb{N}  }} 1\\
& = \frac{1}{N} \sum _{\substack{ p^{\alpha} \leq N \\ p \text{ prime} \\ \alpha \in \mathbb{N}  }} \left\lfloor \frac{N}{p ^{\alpha}}  \right\rfloor \\
\end{align*}

\begin{align*}
    \mathbb{E} \left( Y \right) & = \frac{1}{N} \sum _{\substack{ p^{\alpha} \leq N \\ p \text{ prime} \\ \alpha \in \mathbb{N}  }} \left\lfloor \frac{N}{p ^{\alpha}}  \right\rfloor \\
    & = \frac{1}{N} \sum _{\substack{ p^{\alpha}\leq N \\ p \text{ prime}  \\ \alpha \in \mathbb{N}   }} \left( \frac{N}{p ^{\alpha}} - \delta_{p ^{\alpha}}  \right)  \\
    & = \sum _{\substack{ p^{\alpha}\leq N \\p  \text{ prime} \\ \alpha \in  \mathbb{N}  }} \frac{1}{p ^{\alpha}} - \frac{1}{N} \sum _{\substack{ p ^{\alpha} \leq N \\ p \text{ prime} \\ \alpha \in  \mathbb{N} }}  \delta_{p ^{\alpha}} \\
    & = \log\log \left( N \right) + O(1) - \frac{O(1) \psi(N)}{N} \\
    & = \log\log \left( N \right) + O(1).
\end{align*}


To calculate \(V(Y)\), we have that
\[
    V(Y) = \mathbb{E} (Y^{2}) - \mathbb{E} (Y)^{2} = \mathbb{E} (Y^{2}) - \left( \log\log \left( N \right) + O(1) \right) ^{2}
,\]
so it suffices to calculate
\[
    \mathbb{E} (Y^{2})
.\]

\begin{align*}
\mathbb{E} \left( Y^{2} \right) & = \frac{1}{N} \sum _{x \leq N} ( \sum _{\substack{ p ^{\alpha} \mid x \\ p \text{ prime} \\ \alpha \in  \mathbb{N}  }} 1 ) ^{2} \\
& = \frac{1}{N} \sum _{x \leq N} \sum _{\substack{ (p^{\alpha},q^{\beta}) \\ p,q \text{ prime} \\ \alpha , \beta \in \mathbb{N} \\ p ^{\alpha}, q^{\beta} \mid x }} 1\\
& = \frac{1}{N} \sum _{x \leq N} \sum _{\substack{ (p^{\alpha},q^{\beta})\\ p \neq q \\ p,q \text{ prime} \\ \alpha , \beta \in \mathbb{N} \\ p ^{\alpha}, q^{\beta} \mid x }} 1 + \frac{1}{N} \sum _{x \leq N} \sum _{\substack{ (p^{\alpha},p^{\beta}) \\ p \text{ prime} \\ \alpha , \beta \in \mathbb{N} \\ p ^{\alpha}, p^{\beta} \mid x }} 1\\
& = \frac{1}{N} \sum _{\substack{ \left( p ^{\alpha} ,q^{\beta} \right) \\ p,q \text{ prime} \\ p \neq q \\ p^{\alpha}q^{\beta} \leq N \\ \alpha,\beta \in \mathbb{N}  }} \left\lfloor \frac{N}{p ^{\alpha} q ^{\beta}}  \right\rfloor + \frac{1}{N} \sum _{\substack{ (p ^{\alpha}, p ^{\beta}) \\ p ^{ \max \left( \alpha,\beta \right) }\leq N \\ p \text{ prime} \\ \alpha, \beta \in \mathbb{N}  }} \left\lfloor \frac{N}{p ^{\max \left( \alpha,\beta \right) }}  \right\rfloor \\
\end{align*}

\begin{align*}
    \mathbb{E} \left( Y^{2} \right) & = \frac{1}{N} \sum _{\substack{ \left( p ^{\alpha} ,q^{\beta} \right) \\ p,q \text{ prime} \\ p \neq q \\ p^{\alpha}q^{\beta} \leq N \\ \alpha,\beta \in \mathbb{N}  }} \left\lfloor \frac{N}{p ^{\alpha} q ^{\beta}}  \right\rfloor + \frac{1}{N} \sum _{\substack{ (p ^{\alpha}, p ^{\beta}) \\ p ^{ \max \left( \alpha,\beta \right) }\leq N \\ p \text{ prime} \\ \alpha, \beta \in \mathbb{N}  }} \left\lfloor \frac{N}{p ^{\max \left( \alpha,\beta \right) }}  \right\rfloor \\
    & \leq  \sum _{\substack{ \left( p ^{\alpha} ,q^{\beta} \right) \\ p,q \text{ prime} \\ p \neq q \\ p^{\alpha}q^{\beta} \leq N \\ \alpha,\beta \in \mathbb{N}  }} \frac{1}{ p ^{\alpha} q ^{\beta}} - \frac{1}{N}  \sum _{\substack{ \left( p ^{\alpha} ,q^{\beta} \right) \\ p,q \text{ prime} \\ p \neq q \\ p^{\alpha}q^{\beta} \leq N \\ \alpha,\beta \in \mathbb{N}  }} \delta_{p ^{\alpha}q ^{\beta}} +2 \sum _{\substack{ p ^{\alpha} \leq N \\ \beta \leq \alpha\\ p \text{ prime} \\ \alpha, \beta \in \mathbb{N}  }}\frac{1}{p ^{\alpha}} - \frac{2}{N}  \sum _{\substack{ p ^{\alpha} \leq N \\ \beta \leq \alpha\\ p \text{ prime} \\ \alpha, \beta \in \mathbb{N}  }}\delta_{p ^{\alpha}}   \\
    & \leq \left( \sum _{\substack{ p ^{\alpha} \leq N \\ p \text{ prime} \\ \alpha \in \mathbb{N}  }} \frac{1}{p ^{\alpha}}  \right) ^{2} +  2 \sum _{\substack{ p ^{\alpha} \leq N \\ p \text{ prime} \\ \alpha \in  \mathbb{N} }} \frac{\alpha}{p ^{\alpha}}
\end{align*}


We have that
\begin{align*}
2 \sum _{\substack{ p ^{\alpha} \leq N \\ \alpha\geq 2\\ p \text{ prime} }} \frac{\alpha}{p ^{\alpha}} & \leq 2 \sum _{\substack{ x \geq 2 \\ \alpha \geq 2 }}  \frac{\alpha}{x ^{\alpha}} \\
& = 2\sum _{x \geq 2} \left( x \sum _{\alpha \geq 2} \frac{\alpha}{x ^{\alpha + 1}}  \right) \\
& = 2 \sum _{x \geq 2} \left( x \cdot \frac{\mathrm{d}}{\mathrm{d} x}\left( \sum _{\alpha \geq 2} - \frac{1}{x ^{\alpha}} \right)  \right) \\
& = -2 \sum _{x \geq 2} \left( x \cdot \frac{\mathrm{d}}{\mathrm{d} x}\left( \frac{\frac{1}{x^{2}} }{1 - \frac{1}{x} }  \right)  \right)\\
& = 2 \sum _{x \geq 2}  \frac{x (2x-1)}{\left( x^{2}-x \right) ^{2}}
\end{align*}
which, via limit comparison test with
\[
    \frac{1}{x^{2}} 
,\]
converges.

It follows that
\[
    2 \sum _{\substack{ p ^{\alpha} \leq N \\ p \text{ prime} \\ \alpha \in  \mathbb{N} }} \frac{\alpha}{p ^{\alpha}} = 2 \sum _{\substack{ p \leq N \\ p  \text{ prime} }} \frac{1}{p}  + O(1) = 2 \log\log \left( N \right) + O(1)
,\]
and therefore that
\begin{align*}
\mathbb{E} \left( Y^{2} \right) & \leq \left( \log\log \left( N \right) + O(1) \right)^{2} + 2 \log\log \left( N \right) + O(1)\\
& = \left( \log\log \left( N \right)  + O(1) \right) ^{2}. 
\end{align*}


This gives that
\begin{align*}
V(Y) & = \mathbb{E} \left( Y^{2} \right) - \mathbb{E} (Y)^{2}\\
& \leq \left( \log\log \left( N \right) + O(1) \right) ^{2} - \left( \log\log \left( N \right) + O(1) \right) ^{2}\\
& = O \left( \log\log \left( N \right)  \right).
\end{align*}

Applying the lemma, we see that for any \(\delta > 0\),
\[
    \mathbb{P} \left( \left\lvert Y - \log\log \left( N \right)  \right\rvert \geq \log\log \left( N \right) ^{\delta} \right) \leq \frac{O(\log\log \left( N \right) )}{\log\log \left( N \right) ^{2\delta}} 
,\]
this gives, for any \(\epsilon > 0\)
\[
    \mathbb{P} \left( \left\lvert Y - \log\log \left( N \right)  \right\rvert \geq \log\log \left( N \right) ^{\frac{1}{2} + \epsilon} \right) \leq \frac{O(\log\log \left( N \right) )}{\log\log \left( N \right) ^{1 + \epsilon}} = o(1) 
.\]

We have proven the following statement: For any \(\epsilon > 0\) and \(N \in \mathbb{N} \), all but \(o(N)\) of the
numbers \(x \leq N\) satisfy
\[
    \log\log \left( N \right) - \log\log \left( N \right) ^{\frac{1}{2} + \epsilon} \leq \Omega(x) \leq \log\log \left( N \right) + \log\log \left( N \right) ^{\frac{1}{2} + \epsilon} 
.\]

For products \(ab\), we clearly have that
\[
    \Omega(ab) = \Omega(a) + \Omega(b)
,\]
and so all but \(o(N^{2})\) of the products \(ab\) with \(a,b \leq N\) satisfy
\[
    2\log\log \left( N \right) - 2\log\log \left( N \right) ^{\frac{1}{2} + \epsilon} \leq \Omega(ab) \leq 2\log\log \left( N \right) + 2\log\log \left( N \right) ^{\frac{1}{2} + \epsilon} 
.\]

On the contrary, all but \(o(N^{2})\) of the numbers \(x \leq N^{2}\) satisfy
\[
    \log\log \left( N^{2} \right) - \log\log \left( N^{2} \right) ^{\frac{1}{2} + \epsilon} \leq \Omega(x) \leq \log\log \left( N^{2} \right) + \log\log \left( N^{2} \right) ^{\frac{1}{2} + \epsilon} 
\]
and, for \(\epsilon < \frac{1}{2} \),
\begin{align*}
\lim_{N \to \infty} \frac{\log\log \left( N^{2} \right) + \log\log \left( N^{2} \right) ^{\frac{1}{2} + \epsilon}}{2 \log\log \left( N \right) - 2 \log\log \left( N \right) ^{\frac{1}{2} +\epsilon}} = \frac{1}{2}.
\end{align*}
That is, the majority of products \(ab\) are only a small portion of the total numbers
\(x \leq N^{2}\). Precisely,
\[
    \left\lvert [n] \cdot [n] \right\rvert - o(N^{2}) = o(N^{2}) \implies \left\lvert [n] \cdot [n] \right\rvert = o(N^{2})
.\]

\end{proof}
\end{document}