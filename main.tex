\documentclass[12pt]{amsart}
\input{preamble}
\graphicspath{{Figures/}}

%────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
%────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────

\begin{document}
\title{\textbf{TITLE}} 

%%%% DEPARTMENT OF MATHEMATICS???????? Why not CAS?

\author[Cushman]{Adam Cushman}
\address{Department of Mathematics\\
         Indiana University,
         Bloomington, IN 47405
         USA} 
\email{acushma@iu.edu}
\date{\today}

\begin{abstract}
The sum-product conjecture, posed in 1983 by Erd{\H{o}}s and Szemer{\'e}di \cite{erdos-szemeredi}, 
posits that any sufficiently large set must have a `relatively large' number
of distinct sums or products between its elements.
A newer conjecture extends this idea, positing that for convex sets there must be a `relatively large'
number of distinct sums between its elements. Both conjectures remain open and far from being solved.
This report provides a self-contained overview of some known results and the methods
used to achieve them.
\end{abstract}

\maketitle

%────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
%────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────

\textbf{issue of "for any set A" in statements, but only as A gets sufficiently large}

\section{Introduction and Motivation} 

For any sets \(A,B\) and binary operation \(\cdot \) which acts on elements of \(A\) and \(B\), we define
\[
    A \cdot  B = \left\{ a \cdot  b : a \in A~,~ b \in B \right\} 
.\]

Observe the following example which motivates the study of the sum-product problem.

Let \(A = \left\{ 1, 2, 3, \cdots, n \right\}\) and \(G = \left\{ 2, 2^{2}, 2^{3}, \cdots , 2^{n} \right\}\).

Notice that \(A\) is given by an arithmetic sequence and \(G\) by a geometric sequence.
We are going to calculate \(\left\lvert A+A \right\rvert , \left\lvert AA \right\rvert , \left\lvert G+G \right\rvert ,\) and \(\left\lvert GG \right\rvert \).

Observe that
\begin{align*}
    \left\lvert A + A \right\rvert  & = \left\lvert \left\{2,3, \cdots ,2n \right\}  \right\rvert \\
    & = 2n - 1 \\
    & = 2 \left\lvert A \right\rvert - 1,
\end{align*}
and by the same argument,
\begin{align*}
    \left\lvert GG \right\rvert  & = \left\lvert \left\{2^{2}, 2^{3}, \cdots , 2^{2n} \right\} \right\rvert  \\
    & =2 \left\lvert G \right\rvert -1.
\end{align*}
We have
\begin{align*}
    \left\lvert G + G \right\rvert & = \left\lvert \left\{ 2^{i} + 2^{j} : i,j \in \left\{ 1, \cdots , n \right\} \right\} \right\rvert
\end{align*}
For all \(i,j\), \(2^{i} + 2^{j}\) is a number written in base \(2\). By the uniqueness of representations
in different bases (see appendix), we have that \(2^{i} + 2^{j}\) is distinct for every choice of \(\left\{ i,j \right\} \). Therefore,
\begin{align*}
    \left\lvert G + G \right\rvert & \geq  \left\lvert \left\{ \left\{ i,j \right\} : i,j \in \left\{ 1, \cdots , n \right\}  \right\}  \right\rvert \\
    & = \binom{n}{2} + n \\
    & = \binom{\left\lvert G \right\rvert + 1}{2} 
\end{align*}

Finally, I prove in the appendix that \textbf{PROVE IT IN THE APPENDIX}.

Observing the trivial bounds \( 2 \left\lvert S \right\rvert - 1 \leq \left\lvert S \cdot S \right\rvert \leq \binom{\left\lvert S \right\rvert }{2} \) for any set \(S\) and any
commutative operation \(\cdot \), it is clear that both \(AA\) and \(G +G\) are almost as large as they can be,
and \(A + A, GG \) are as small as they can be.

This is the phenomenon which motivates this problem.
One questions is: ``does there exist a set for which both the sum and product sets are small?''
The sum-product conjecture states that such a set does not exist. 
Another question we study in this report is: ``what
determines if the sum set is large or the product set is large?'' A
conjecture which tries to partially answer this question states that the sum set is large
when the set itself is convex.

The rest of the report will proceed in the following way : \textbf{finish this}

\section{Preliminaries}

As a useful shorthand, for any natural number \(n\), let
\[
    [n] = \left\{ 1, 2 , \cdots , n \right\} 
.\]

The study of these problems requires the notion of orders of magnitude.
For any functions \(f,g: \mathbb{R}   \to \mathbb{R} \), write
\[
    f(x) \gg g(x) \text{ as } x \to \infty 
\]
if
\[
    \exists x_0,c \in \mathbb{R} ^{+}~~\text{s.t.}~~ x > x_0 \implies \left\lvert f(x) \right\rvert \geq c \left\lvert g(x) \right\rvert  
,\]
write
\[
    f(x) \ll g(x) \text{ as } x \to \infty 
\]
if
\[
     \exists x_0,c \in \mathbb{R} ^{+}~~\text{s.t.}~~ x > x_0 \implies \left\lvert f(x) \right\rvert \leq c \left\lvert g(x) \right\rvert 
,\]
and write
\[
    f(x) \asymp g(x) \text{ as } x \to \infty 
\]
if
\[
    f(x) \ll g(x)  ~ ~ \text{and} ~ ~ f(x) \gg g(x) \text{ as } x \to \infty 
.\]

We write \(\ll _{\epsilon},\gg _{\epsilon}, \asymp _{\epsilon} \) if the constant depends on \(\epsilon\). For example,
\[
    f(x) \gg _{\epsilon} g(x) ^{ \epsilon}
\]
if there is some function \(c : \mathbb{R} ^{+ }\to \mathbb{R} ^{+}\) so
\[
    \forall \epsilon > 0, \exists x_0\in \mathbb{N} ~~\text{s.t.}~~ x > x_0 \implies \left\lvert f(x) \right\rvert \geq c (\epsilon)\left\lvert g(x) \right\rvert ^{\epsilon}
.\]
This should be interpreted as
\[
   \forall \epsilon> 0~,~ f(x) \gg g(x) ^{\epsilon}
.\]

Throughout this report, the asymptotic parameter (in this case \(x\)) will always tend to infinity,
so it will no longer be mentioned.
Oftentimes the parameter will not even be in the expression. For example,
if for some set \(A\) we write
\[
    \left\lvert A + A \right\rvert \gg \left\lvert A \right\rvert 
,\]
it is taken to mean that \(A\) is defined implicitly by \(\left\lvert A \right\rvert \), and
\(\left\lvert A \right\rvert \) is the parameter which tends to \(\infty \).

For any sets \(A,B\) and any binary operation \(\cdot \) acting on elements of \(A\) and \(B\), define the representation function
\(r_{A \cdot B} : A \cdot B \to \mathbb{N} \) by
\[
    r_{A \cdot B} (x) = \left\lvert \left\{ (a,b)\in  A \times B : x = a \cdot b \right\}  \right\rvert
.\]

Throughout this report the shorthand
\[
    \delta_{A,B} (x) = r_{A - B} (x)~,~ \sigma _{A,B} (x) = r_{A + B} (x)~,~ \delta_{A} (x) = \delta_{A,A} (x) ~,~ \sigma_{A} (x) = \sigma _{A,A} (x)
\]
will be used.

For any sets \(A,B\), define the Additive Energy \(E(A,B)\) and Multiplicative Energy \(M(A,B)\) by
\[
    E(A,B) = \left\lvert \left\{ (a_1,a_2,b_1,b_2) \in A^{2} \times B^{2} : a_1 - b_1 = a_2 - b_2 \right\}  \right\rvert 
\]
and
\[
    M(A,B) = \left\lvert \left\{ (a_1,a_2,b_1,b_2)\in  A^{2} \times B^{2} : \frac{a_1}{b_1}  = \frac{a_2}{b_2}  \right\}  \right\rvert
.\]

Observe that
\[
    E(A,B) = \sum _{x \in A - B} \delta_{A,B} (x)^{2}
\]
and
\[
    M(A,B) = \sum _{x \in \frac{A}{B} } r_{\frac{A}{B} } (x)^{2} 
.\]

This definition is symmetric in the sense that a 4-tuple \((a_1,a_2,b_1,b_2)\in A^{2} \times B^{2}\) is a solution to
\[
    a_1 - b_1 = a_2 - b_2
\]
if and only if it is a solution to
\[
    a_1+b_2 = a_2+b_1
,\]
and therefore
\[
    E(A,B) = \sum _{x \in A + B} \sigma_{A,B}  (x)^{2} = \sum _{x \in A - B} \delta_{A,B}  (x)^{2}
.\]

\textbf{^^ switch order}

There is a similar argument for multiplicative energy.
Any 4-tuple \((a_1,a_2,b_1,b_2) \in A^{2} \times B^{2}\) with nonzero entries is a solution to
\[
    \frac{a_1}{b_1} = \frac{a_2}{b_2}
\]
if and only if it is a solution to
\[
    a_1 b_2 = a_2 b_1
.\]
There are at most
\[
    \sum _{i = 1} ^{4} \binom{4}{i}  = 15
\]
4-tuples with zero entries, so
\[
    M(A,B) = \sum _{x \in \frac{A}{B} } r_{\frac{A}{B} } (x) ^{2} \asymp \sum _{x \in AB} r_{AB} (x)^{2}
.\]

We also define higher energies
\[
    E_{n} (A,B) = \sum _{x \in A-B} \delta_{A,B} (x)^{n}  
,\]
so
\[
    E(A) = E_{2} (A)
,\]
and as a shorthand use
\[
    E_{n} (A) = E_{n} (A,A)
.\]
Similar definitions and shorthand are used for multiplicative energy.

We can relate the energies to the sizes of the sum and product sets by the Cauchy-Schwarz Inequality.
\[
    \left\lvert A \right\rvert \left\lvert B \right\rvert = \sum _{x \in A + B} \sigma_{A,B}  (x) \leq \left\lvert A + B \right\rvert ^{\frac{1}{2} } E(A,B)^{\frac{1}{2} }
,\]
and
\[
    \left\lvert A \right\rvert \left\lvert B \right\rvert = \sum _{x \in AB} r_{AB} (x) \leq \left\lvert AB \right\rvert ^{\frac{1}{2} }M(A,B)^{\frac{1}{2} }
.\]
Similar inequalities can be derived for \(\left\lvert A-B \right\rvert \) and \(\left\lvert \frac{A}{B}  \right\rvert \).

Let \(I \subset \mathbb{R} \) be an interval. 
    We call a function \(f: I \to \mathbb{R} \) convex if for
    any 2 points \(x_1,x_2 \in I\), and any \(\lambda \in [0,1]\),
    \[
        f((x_1-x_2)\lambda + x_2) \leq \left( f(x_1)-f(x_2) \right) \lambda + f(x_2)
    .\]

    A finite set \(A \subset \mathbb{R}\) is convex if there is a function \(f: [1, \left\lvert A \right\rvert ] \to \mathbb{R} \)
such that
\[
    A = \left\{ f(i) : i \in \left\{ 1, \cdots , \left\lvert A \right\rvert  \right\}  \right\} 
.\]

A property of convex functions which will come up later, and is worth proving now, is
\begin{lemma}
Let \(I \subset \mathbb{R} \) be an interval. Let \(f : I \to \mathbb{R} \) be convex. Let \(T \subset \mathbb{R} ^{2}\).

Take \(\ell  = \left\{ (x,f(x)) : x \in I \right\} \) to be the graph of \(f\). We have that
\[
    \forall t_1,t_2 \in T ~,~ \left\lvert \left( \ell + t_1 \right) \cap \left( \ell +  t_2 \right)  \right\rvert   \leq  1
.\]

That is that translations of the graph of a convex function intersect in at most one point.
\end{lemma}

\begin{proof}
\textbf{PROVE THIS}
\end{proof}

\textbf{Dyadic partitioning introduction}

%────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
%────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────


\section{Statement of Problems}

We'll now move on to precise statements of the mentioned problems, and statements of the most modern results.

The idea that there does not exist a set with a small sum and product set is
stated precisely as
\begin{conjecture}[Sum-Product Conjecture]
For every finite set \(A \subset \mathbb{R} \),
\[
    \max \left( \left\lvert A+A \right\rvert, \left\lvert A \cdot A \right\rvert  \right) \gg_{\epsilon}  \left\lvert A \right\rvert^{2-\epsilon}
\]
\end{conjecture}

The idea that convexity opposes additive structure is stated precisely as

\begin{conjecture}[Convex Sumset Conjecture]
    For finite and convex set \(A\),
    \[
        \left\lvert A+A \right\rvert \gg_{\epsilon}  \left\lvert A \right\rvert ^{2-\epsilon}
    \]
\end{conjecture}

To date, the best results for both of these conjectures are proven in \cite{stevens-rudnev}.
\begin{theorem}
    For every sufficiently large finite set \(A \subset \mathbb{R} \),
    \[
        \max \left( \left\lvert A+A \right\rvert ~,~ \left\lvert A \cdot A \right\rvert  \right) \gg_{\epsilon}   \left\lvert A \right\rvert^{\frac{4}{3} + \frac{2}{1167} - \epsilon}
    \]
\end{theorem}
and
\begin{theorem}
    For every sufficiently large, finite, and convex set \(A\),
    \[
        \left\lvert A+A \right\rvert \gg_{\epsilon}  \left\lvert A \right\rvert ^{\frac{30}{19} - \epsilon }
    \]
\end{theorem}

These results will not be covered in this report.
\textbf{best in this report is ... ,}

\textbf{WHAT I DO COVER IN THIS REPORT IS ...}

\section{Graphs and the Crossing Number Inequality}

\textbf{simple, connected}

\textbf{Tao's article reread to see what I missed}

\textbf{First proven in ...}

\textbf{Theres something going on with connectedness here}

%────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
%────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────

\textbf{rewrite this paragraph}

A profoundly useful theorem in the study of sum-product conjectures is the
Szemeredi-Trotter theorem, a statement about systems of points and lines.

The easiest way to prove this theorem is through the use of the crossing number inequality,
a statement about how planar a given graph can be. The purpose of this section is to give a proof
of the crossing number inequality.

Due to the brevity of this report, the definitions and proofs given will not be
as rigorous as they should be. There are also pictures to help understand.

%────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
%────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────


\textbf{define connected}

A graph \(G\) is a pair \(G = (V,E)\) where each \(e \in E\) is of the form \(e \subset V\) with \(\left\lvert e \right\rvert = 2\).
We call the set \(V\) the vertices, and the set \(E\) the edges. A drawing is
a representation of a graph with vertices as points in the plane and edges as curves between their
respective vertices. For example:

\begin{figure}[h]
    \centering
    \includegraphics[width=0.1\textwidth]{3graph.png}
    \caption{A drawing of a graph with \(V = \left\{ A,B,C \right\}\) and \(E = \left\{ \left\{ A,B \right\} , \left\{ B,C \right\} ,\left\{ A,C \right\}  \right\} \)}
\end{figure}

A graph is connected if for any 2 vertices, there exists a sequence of edges which join them.
Note that connectedness is a property of a graph and not of the drawing of a graph.

There are infinitely many ways to draw any given graph. A crossing in a drawing
of a graph is an intersection between 2 curves which represent edges. The 
crossing number of a graph is the minimum number of crossings a drawing
of the graph can have. Denote this by
\(\crossing(G)\). A graph \(G\) is called planar if its crossing number is 0.

A precise statement of the Crossing Number Inequality is

\begin{theorem}\label{thm:crossing-number-inequality}
    Let \(G = (V,E)\) be a connected graph. If \(\left\lvert E \right\rvert \geq 4 \left\lvert V \right\rvert \)
    then
    \[
        \crossing\left( G \right) \gg \frac{\left\lvert E \right\rvert ^{3} }{\left\lvert V \right\rvert ^{2}}  
    .\]
\end{theorem}

For a drawing of a planar graph, we call any region of the plane which is bounded by edges a face. We also call
the unbounded region of the plane a face. Here is an example of a drawing of a planar
graph with labelled faces:

\begin{figure}[h]
    \centering
    \includegraphics[width=0.4\textwidth]{faceimage.png}
    \caption{Drawing of Planar Graph with Labeled Faces \(f_{i} \)}
\end{figure}

\textbf{cite tao }

Observe that any non-planar graph \(G = (V,E)\) can be turned into a planar graph by removing at most
\(\crossing\left( G \right) \) edges from \(E\). Therefore, a bound on the number of
edges a graph can have and remain planar yields a bound on the crossing number for any graph.
A famous theorem relating the vertices and edges of planar graphs is

\begin{theorem}[Euler's Formula for Planar Graphs]\label{thm:euler-formula-graphs}
Let \(G = (V,E)\) be a connected planar graph, with \(\left\lvert V \right\rvert \geq 1\), and consider some drawing with 0 crossings.
Let \(F\) be the set of all faces of this drawing.
\[
    \left\lvert V \right\rvert - \left\lvert E \right\rvert + \left\lvert F \right\rvert = 2
.\]
\end{theorem}

\begin{proof}
We may construct our drawing of \(G\) by first drawing a vertex, and then doing combination of the following steps: 

\textbf{PROOF}
\end{proof}

The dependence on \(\left\lvert F \right\rvert \) in Euler's formula can be removed by using
its obvious dependence on \(\left\lvert E \right\rvert \). 

We call an edge incident to a face if the edge is one of the bounding edges which define the face. 
Define \(\chi : F \times E \to \left\{ 0,1 \right\} \) as the incidence function, so
\(\chi(f,e) = 1\) if \(f\) and \(e\) are incident, and \(\chi(f,e) = 0\) otherwise.
The total number of face edge incidences is
\[
    I(F,E) = \sum_{f\in F} \sum _{e \in E} \chi(f,e)
.\]

\textbf{JUSTIFY THIS ASSUMPTION SOMEWHERE}
We assume \(\left\lvert E \right\rvert  \geq 3\), so that every face is incident to at least 3 edges.
It follows that
\[
    I \geq \sum _{f \in F} 3 = 3 \left\lvert F \right\rvert 
.\]
Every edge is incident to at most 2 faces, so it follows that
\[
    I \leq \sum _{e \in E} 2 = 2 \left\lvert E \right\rvert 
.\]
Therefore
\[
    3 \left\lvert F \right\rvert \leq 2 \left\lvert E \right\rvert 
\]
or
\[
    \left\lvert F \right\rvert \leq \frac{2}{3} \left\lvert E \right\rvert 
.\]
Applying this to Euler's formula,
\[
    \left\lvert V \right\rvert - \left\lvert E \right\rvert + \frac{2}{3} \left\lvert E \right\rvert \geq 2
\]
or
\[
    \left\lvert E \right\rvert \leq 3 \left\lvert V \right\rvert -6
\]
when \(\left\lvert E \right\rvert \geq 3\).
Now suppose that \(G = (V,E)\) is non-planar and connected. As mentioned before, \(G\) may be turned planar by removing
at most \(\crossing\left( G \right) \) edges.

\textbf{READ TAO AND JUSTIFY ABOVE BETTER}
Therefore, for any graph \(G\) with \(\left\lvert E \right\rvert  \geq 3\),
\[
    \left\lvert E \right\rvert - \crossing\left( G \right) \leq 3 \left\lvert V \right\rvert - 6
\]
or
\[
    \crossing\left( G \right) > \left\lvert E \right\rvert - 3 \left\lvert V \right\rvert 
.\]

To further improve this inequality, we apply the probabilistic method
to the deletion of vertices of \(G\).

Let each \(v \in V\) be
removed with a probability \(1-p~,~  p \in (0,1)\). Let the remaining set of vertices be \(V'\). 

An edge is
removed whenever either of the corresponding vertices are removed. Let the remaining set of edges be \(E'\). Let the remaining graph be
\(G' = (V',E')\). We have that if \(\left\lvert E' \right\rvert \geq 3\),
\[
    \crossing\left( G' \right) \geq \left\lvert E' \right\rvert - 3 \left\lvert V' \right\rvert 
,\]
and so
\[
    \mathbb{E} \left( \crossing\left( G' \right)  \right) \geq \mathbb{E} \left( \left\lvert E' \right\rvert - 3 \left\lvert V' \right\rvert  \right) 
,\]
or, by the linearity of the expected value,
\[
    \mathbb{E} \left( \crossing\left( G' \right)  \right) \geq \mathbb{E} \left( \left\lvert E' \right\rvert  \right) - 3 \mathbb{E} \left( \left\lvert V' \right\rvert  \right)    
.\]
Each \(v \in V\) is removed with probability \(1-p\), so
\[
    \mathbb{E} \left( \left\lvert V' \right\rvert  \right) = p \left\lvert V \right\rvert 
.\]
Each edge remains only when both corresponding vertices remain. Each vertex remains
independently with a probability \(p\), so 
\[
    \mathbb{E} \left( \left\lvert E' \right\rvert  \right) = p^{2}\left\lvert E \right\rvert 
.\]

We can bound \(\mathbb{E} \left( \crossing\left( G' \right)  \right) \) by considering a drawing of \(G\) with the minimum number of crossings. 
Each crossing remains and only when both corresponding edges remain, each of which occurs independently with probability
\(p^{2}\).

The expected value of the number of crossings remaining in the drawing is \(p^{4} \crossing\left( G \right) \).
There is no guarantee that this drawing is optimal to minimize the crossings of \(G'\), but we may conclude that
\[
    \mathbb{E} \left( \crossing\left( G' \right)  \right) \leq p ^{4} \crossing\left( G \right)
,\]
and therefore that
\[
    p^{4} \crossing\left( G \right) \geq \mathbb{E} \left( \crossing\left( G \right)  \right) \geq p^{2} \left\lvert E \right\rvert - 3 p \left\lvert V \right\rvert 
\]
for any \(p \in (0,1)\). 


Because we are only concerned with an order of magnitude bound, we assume \(\left\lvert E \right\rvert \geq 4\left\lvert V \right\rvert \), and take \(p = \frac{4\left\lvert V \right\rvert }{\left\lvert E \right\rvert }\).
This yields the result
\[
    \crossing\left( G \right) \geq \frac{\left\lvert E \right\rvert }{\left( \frac{4 \left\lvert V \right\rvert }{\left\lvert E \right\rvert }   \right)^{2} } - \frac{3 \left\lvert V \right\rvert }{\left( \frac{4 \left\lvert V \right\rvert }{\left\lvert E \right\rvert }  \right) ^{3} } = \frac{1}{16} \left( \frac{\left\lvert E \right\rvert ^{3} }{\left\lvert V \right\rvert ^{2}} - \frac{3 \left\lvert E \right\rvert ^{3} }{4 \left\lvert V \right\rvert ^{2}}  \right) \gg \frac{\left\lvert E \right\rvert ^{3} }{\left\lvert V \right\rvert ^{2}}
.\]

\section{The Szemeredi-Trotter Theorem}

With this we can quite easily prove the Szemeredi-Trotter Theorem.
A precise statement of the Szemeredi-Trotter Theorem is

\textbf{First proven in ...}

\begin{theorem}[Szemeredi-Trotter Theorem]
Let \(P \subset \mathbb{R} ^{2}\) be a finite set of points. Let \(\mathcal{L} \) be a finite set of curves in \(\mathbb{R} ^{2}\). 

More precisely,
every \(l \in \mathcal{L} \) is of the form \(l = \left\{ (x(t),y(t)) : t\in \mathbb{R}  \right\} \) for some \(x,y \in C^{0}(\mathbb{R} )\). 

Let \(\chi : P \times \mathcal{L}  \to \left\{ 0,1 \right\} \) be the incidence function
between a point and a line, so
\[
\chi(p,l) = 
\begin{cases}
1 ~ ~ \text{if} ~ ~ p\in l\\
0 ~ ~ \text{otherwise} 
\end{cases}
\]

If any two \( l \in \mathcal{L} \) intersect in at most one point,
then the total number of point-line incidences,
\[
    I(P, \mathcal{L} ) = \sum _{(p,l)\in P \times \mathcal{L} } \chi(p,l)
\]
satisfies
\[
    I(P,\mathcal{L} ) \ll  \left\lvert P \right\rvert^{\frac{2}{3} } \left\lvert \mathcal{L}  \right\rvert ^{\frac{2}{3} } + \left\lvert P \right\rvert + \left\lvert \mathcal{L}  \right\rvert 
.\]
\end{theorem}

\begin{proof}
This is proven by turning a system of curves and points into a graph. We first omit all points and curves which
contribute to one or fewer incidences. 

For each remaining curve, if there are \(n\) incidences
along it, we partition it into \(n-1\) curves, each with 2 incidences. These become
the edges of the drawing of some graph. Let the set of edges be \(E\).

If \(I_0(P , \mathcal{L} )\) is the remaining number of incidences,
\[
    \left\lvert E \right\rvert \geq  I_0(P , \mathcal{L} ) - \left\lvert \mathcal{L}  \right\rvert 
.\]

Let the vertices of the graph, \(V\), be the remaining points in the system. Obviously, \(\left\lvert V \right\rvert \leq \left\lvert P \right\rvert \).

Because any two curves intersect in at most one point, \(\crossing\left( G \right) \) is at most
\(\left\lvert \mathcal{L}  \right\rvert ^{2}\).

Supposing that \(\left\lvert E \right\rvert \geq 4 \left\lvert V \right\rvert \), we have that
\[
    \left\lvert \mathcal{L}  \right\rvert ^{2} \geq \crossing\left( G \right) \gg \frac{\left( I_0 \left( P, \mathcal{L}  \right) - \left\lvert \mathcal{L}  \right\rvert \right)  ^{3}  }{\left\lvert P \right\rvert ^{2}} 
\]
or
\[
    I_0(P , \mathcal{L} ) \ll \left\lvert \mathcal{L}  \right\rvert ^{\frac{2}{3} } \left\lvert P \right\rvert ^{\frac{2}{3} } + \left\lvert \mathcal{L}  \right\rvert 
.\]

If \(\left\lvert V \right\rvert \geq \frac{1}{4} \left\lvert E \right\rvert \), then \(\left\lvert V \right\rvert  \gg \left\lvert E \right\rvert \), or
\[
    \left\lvert P \right\rvert \gg I_0\left( P , \mathcal{L}  \right) - \left\lvert \mathcal{L}  \right\rvert \implies I_0(P,\mathcal{L} ) \ll \left\lvert P \right\rvert  + \left\lvert \mathcal{L}  \right\rvert  
.\]

Finally, the remaining incidences not yet counted is 
\[
    I(P, \mathcal{L} ) - I_0 (P , \mathcal{L} ) \ll \left\lvert \mathcal{L} \right\rvert + \left\lvert P \right\rvert
,\]
so
\[
    I(P, \mathcal{L} ) \ll \left\lvert P \right\rvert ^{\frac{2}{3} }\left\lvert \mathcal{L}  \right\rvert ^{\frac{2}{3} } + \left\lvert P \right\rvert + \left\lvert \mathcal{L}  \right\rvert 
.\]
\end{proof}

We'll now go over some direct applications of the Szemeredi-Trotter theorem
to sum product theorems. The main theorem which does this is proven in \cite{elekes}.

\begin{theorem} \label{thm:translations-of-convex-szt}
For some \(n \in \mathbb{N} \), let \(f : [1,n] \to \mathbb{R} \) be convex.

Let \(S = \left\{ (i,f(i)) : i \in [n] \right\} \) and \(T \subset \mathbb{R} ^{2}\) be finite. We have
\[
    \left\lvert S + T \right\rvert \gg \max \left( \left\lvert S \right\rvert ^{\frac{3}{2} } \left\lvert T \right\rvert ^{\frac{1}{2} } , \left\lvert S \right\rvert \left\lvert T \right\rvert  \right)
.\]
\end{theorem}

\begin{proof}
    Let
    \[
        L_{t} = \left\{ (x,f(x)) + t : x \in [1,n], t \in T \right\} 
    ,\]
    and let
    \[
        \mathcal{L} = \left\{ L_{t} : t \in T \right\} 
    .\]
    For every \(x \in [n]\), \((x,f(x)) + t \in S + T\). Therefore, there are \(\left\lvert S \right\rvert \) incidences
    between \(L_{t} \) and the point set \(S + T\) for all \(t \in T\). The set \(\mathcal{L} \) consists of
    translations of the graph of a convex function, so the Szemeredi-Trotter theorem is satisfied.
    
    \[
        \left\lvert S \right\rvert \left\lvert T \right\rvert \ll \left\lvert S + T \right\rvert ^{\frac{2}{3} } \left\lvert T \right\rvert ^{\frac{2}{3} } + \left\lvert S + T \right\rvert + \left\lvert T \right\rvert 
    .\]
    Trivially,
    \[
        \left\lvert S + T \right\rvert \geq \left\lvert T \right\rvert 
    ,\]
    so
    \[
        \left\lvert S \right\rvert \left\lvert T \right\rvert  \ll\max \left( \left\lvert S + T \right\rvert ^{\frac{2}{3} }\left\lvert T \right\rvert ^{\frac{2}{3} }, \left\lvert S+T \right\rvert  \right) 
    ,\]
    or
    \[
        \left\lvert S + T \right\rvert \gg \max \left( \left\lvert S \right\rvert ^{\frac{3}{2} } \left\lvert T \right\rvert ^{\frac{1}{2} }, \left\lvert S \right\rvert \left\lvert T \right\rvert  \right) 
    .\]
\end{proof}

\textbf{this theorem very useful blah blah blah}
The following corollaries are also proven in \cite{elekes}.

\begin{theorem}
For sufficiently large sets \(A \subset \mathbb{R} \), which are finite and convex, and any set \(B \subset \mathbb{R} \) with \(\left\lvert A \right\rvert = \left\lvert B \right\rvert \),
\[
    \left\lvert A + B \right\rvert \gg \left\lvert A \right\rvert ^{\frac{3}{2} }
.\]
\end{theorem}

\begin{proof}
Let \(\left\lvert A \right\rvert = n\) and \(f : [1,n] \to \mathbb{R} \) be the convex function
such that
\[
    A = \left\{ f(i): i \in [n] \right\}
.\]
Take
\[
    S = \left\{ (i,f(i)) : i \in [n] \right\} 
\]
and
\[
    T = [n] \times B
.\]
Observe that \(S + T \subset [2n] \times (A + B)\), so \(\left\lvert S + T \right\rvert \ll \left\lvert A \right\rvert \left\lvert A + B \right\rvert \). Applying Theorem \ref{thm:translations-of-convex-szt},
\[
    \left\lvert A \right\rvert \left\lvert A + B \right\rvert \gg \max \left( \left\lvert S \right\rvert ^{\frac{3}{2} }\left\lvert T \right\rvert ^{\frac{1}{2} } , \left\lvert S \right\rvert \left\lvert T \right\rvert  \right) = \max \left( \left\lvert A \right\rvert ^{\frac{3}{2} }\left\lvert A \right\rvert , \left\lvert A \right\rvert ^{3} \right) = \left\lvert A \right\rvert ^{\frac{5}{2} }
,\]
so
\[
    \left\lvert A + B \right\rvert \gg \left\lvert A \right\rvert ^{\frac{3}{2} }
.\]
\end{proof}

\textbf{better dialogue}
Another variation of this theorem is

\begin{theorem}
For finite sets \(A \subset \mathbb{R} \) which are convex and sufficiently large, and
any set \(B \subset \mathbb{R} \),
\[
    \left\lvert A + B \right\rvert \gg \left\lvert A \right\rvert \left\lvert B \right\rvert ^{\frac{1}{2} }
.\]
\end{theorem}

\begin{proof}
\textbf{PROOF}
\end{proof}

In particular, both of these theorems give the result
\begin{theorem}
For sufficiently large sets \(A \subset \mathbb{R} \) which are finite and convex,
\[
    \left\lvert A+A \right\rvert \gg \left\lvert A \right\rvert ^{\frac{3}{2} }
.\]
\end{theorem}


\begin{theorem}
    For sufficiently large sets \(A \subset \mathbb{R} \),
    \[
        \max \left( \left\lvert A+A \right\rvert, \left\lvert A\cdot A \right\rvert  \right) \gg \left\lvert A \right\rvert ^{\frac{5}{4} }
    .\]
\end{theorem}

\begin{proof}
Take \(P = \left( A + A \right) \times \left( A \cdot A \right) \). 

For any \(a,b \in A\), let \(\ell _{a,b} : \mathbb{R} \to \mathbb{R} \) be defined by
\(\ell _{a,b} (x) = (x-a)\cdot b\). Let \(L_{a,b} = \left\{ (x, \ell _{a,b} (x)) : x \in \mathbb{R}  \right\} \)
be the graph of \(\ell _{a,b} \).

Take \(\mathcal{L} = \left\{ L_{a,b} : a,b \in A \right\} \). There are \(\left\lvert A \right\rvert \) many numbers in
\(A + A\) of the form \(x + a\) where \(x \in A\). For all of these numbers, \(\ell _{a,b} (x + a) = xb \in A\cdot A\).

We have shown that for each choice of \(a,b \in A\), there are \(\left\lvert A \right\rvert \) many numbers
\(z \in A+A\) such that \(\ell _{a,b} (z) \in A\cdot A\), or that there are \(\left\lvert A \right\rvert \) many
incidences between \(P\) and \(L_{a,b} \). It follows that there are \(\left\lvert A \right\rvert ^{3} \) total incidences
between \(\mathcal{L} \) and \(P\). Because \(\mathcal{L} \) is a set of lines, the Szemeredi-Trotter theorem holds.

\[
    \left\lvert A \right\rvert ^{3} \ll \left\lvert A + A \right\rvert ^{\frac{2}{3} } \left\lvert A \cdot A \right\rvert ^{\frac{2}{3} } \left\lvert A \right\rvert ^{\frac{4}{3} } + \left\lvert A + A \right\rvert \left\lvert A \cdot A \right\rvert + \left\lvert A \right\rvert ^{2}
\]
or
\[
    \left\lvert A \right\rvert ^{3} \ll \max \left( \left\lvert A + A \right\rvert ^{\frac{2}{3} } \left\lvert A \cdot A \right\rvert ^{\frac{2}{3} } \left\lvert A \right\rvert ^{\frac{4}{3} } , \left\lvert A + A \right\rvert \left\lvert A \cdot A \right\rvert , \left\lvert A \right\rvert ^{2} \right) 
.\]

Applying trivial inequalities,
\[
    \left\lvert A \right\rvert ^{2} \leq \left\lvert A+A \right\rvert \left\lvert A \cdot A \right\rvert \leq \left\lvert A+A \right\rvert \left\lvert A \cdot A \right\rvert \left( \frac{\left\lvert A \right\rvert ^{\frac{4}{3} }}{\left\lvert A+A \right\rvert ^{\frac{1}{3} }\left\lvert A \cdot A \right\rvert ^{\frac{1}{3} }}  \right) 
,\]
so
\begin{align*}
    \left\lvert A \right\rvert ^{3} \ll \left\lvert A+A \right\rvert ^{\frac{2}{3} }\left\lvert A \cdot A \right\rvert ^{\frac{2}{3} } \left\lvert A \right\rvert ^{\frac{4}{3} } & \implies \max \left( \left\lvert A + A \right\rvert , \left\lvert A \cdot A \right\rvert  \right) ^{\frac{4}{3} } \gg \left\lvert A \right\rvert ^{\frac{5}{3} } \\
    & \implies \max \left( \left\lvert A + A \right\rvert , \left\lvert A \cdot A \right\rvert  \right) \gg \left\lvert A \right\rvert ^{\frac{5}{4} }.
\end{align*}

\end{proof}

The Szemeredi-Trotter theorem also provides more general tools which can be
used alongside other results to sharpen the above bounds.

\begin{theorem}

    Let \(A \subset \mathbb{R} \) be a sufficiently large convex and finite set, then for every 
    finite set \(B \subset \mathbb{R} \) we have

    \[
        \left\lvert \left\{ x \in A-B : \delta_{A,B} (x) \geq \tau \right\}  \right\rvert \ll \frac{\left\lvert A \right\rvert \left\lvert B \right\rvert^{2} }{\tau^{3} }
    .\]
\end{theorem}

\begin{proof}
\textbf{PROOF}
\end{proof}

An immediate corollary of this is

\begin{corollary}
Let \(A \subset \mathbb{R} \) be a convex and finite set. Let \(B \subset \mathbb{R} \) be finite.
Order elements \(s_{i} \in A-B\) such that 
\[
    \delta_{A,B} (s_1) \geq \delta_{A,B} (s_2) \geq \cdots \geq \delta_{A,B} (s _{\left\lvert A - B \right\rvert } )
.\]
For every \(1 \leq r \leq \left\lvert A - B \right\rvert\) we have
\[
    \delta_{A,B} (s _{r} ) \ll \frac{\left\lvert A \right\rvert^{\frac{1}{3} } \left\lvert B \right\rvert ^{\frac{2}{3} } }{r^{\frac{1}{3} }}
.\]
\end{corollary}

\begin{proof}
\[
    r = \left\lvert \left\{ x \in A-B : \delta _{A,B} (x) \geq \delta_{A,B} (s _{r} ) \right\}  \right\rvert \ll \frac{\left\lvert A \right\rvert \left\lvert B \right\rvert ^{2} }{\delta_{A,B} (s _{r} )^{3} } \implies \delta_{A} (s _{r} ) \ll \frac{\left\lvert A \right\rvert ^{\frac{1}{3} } \left\lvert B \right\rvert ^{\frac{2}{3} }}{r^{\frac{1}{3} }}
.\]
\end{proof}

Later in the next section we'll use these results to find bounds on the additive
energies between certain sets, but first we'll introduce some simpler results which
use additive and multiplicative energy to show why such bounds are useful.

\section{Additive and Multiplicative Energy \textbf{rename section}}

Recall that
\[
    \left\lvert A + A \right\rvert \geq  \frac{\left\lvert A \right\rvert ^{4}}{ E(A) }
\]
and
\[
    \left\lvert AA \right\rvert \geq \frac{\left\lvert A \right\rvert ^{4}}{M(A) }
.\]

Observe that finding an upper bound on \(E(A)\) or \(M(A)\) in terms of \(\left\lvert A + A \right\rvert, \left\lvert AA \right\rvert    \), and \(\left\lvert A \right\rvert \)
yields a sum product theorem. This section consists of results which employ this general idea.

The simplest of which is a result in \cite{Solymosi}, which gives a stronger result
on the sum-product conjecture than the Szemeredi-Trotter theorem. 

\begin{theorem}
Let \(A \subset \mathbb{R}^{+} \) be finite and sufficiently large.
\[
    \max \left( \left\lvert A+A \right\rvert , \left\lvert AA \right\rvert  \right) \gg \left\lvert A \right\rvert ^{\frac{4}{3} - o(1)}
.\]
\end{theorem}

\begin{proof}

\textbf{FIX THIS BEGINNING PART}

\textbf{THE WHOLE THING NEEDS REVISION}

We begin with a construction. Consider the set \(A^{2}\), along with the smallest set of lines
through the origin which cover \(A^{2}\).

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{Sol1.png}
    \caption{Example with \(A = \left\{ 1,2,4,8 \right\} \).}
\end{figure}

Two pairs \((a_1,a_2),(b_1,b_2) \in A^{2}\) give the same representation as a quotient if
and only if
\[
    \frac{a_2}{a_1} = \frac{b_2}{b_1} 
.\]
Observe that this is the slope of a line through the origin and the points \((a_1,a_2), (b_1,b_2)\).

It follows that \(\left\lvert \frac{A}{A} \right\rvert \) is the number of lines through the origin
necessary to cover the point set \(A^{2}\). The slope of the line is the value the line represents 
in \(\frac{A}{A} \), and the number of points on the line is the number of representations of that number in \(\frac{A}{A} \).

Consider 2 consecutive lines and the set of all vector sums between a point on each line.
If our points are \(\left( a_1,a_2 \right)\) and \(\left( b_1,b_2 \right) \), with
\[
    \frac{a_2}{a_1} > \frac{b_2}{b_1}
,\]
then the slope of their sum is
\[
    \frac{a_2+b_2}{a_1+b_1}
\]
which satisfies
\[
    \frac{b_2}{b_1} < \frac{a_2+b_2}{a_1+b_1} < \frac{a_2}{a_1}
.\]
That is, the vector sum must ``lie between'' the two lines which the original vectors are on.
More precisely, for any pairs of consecutive lines, the vector sums of all points along the lines are disjoint.

We also have that the sums of any 2 points on each line are distinct. This is because a
solution to
\[
    \lambda_1 v + \lambda_2 w = \lambda_3 v + \lambda_4 w \iff \left( \lambda_1 - \lambda_3 \right) v + \left( \lambda_2 - \lambda_4 \right) w
\]
where \(\lambda_1 \neq \lambda_3\) or \(\lambda_2 \neq \lambda_4\) exists only if \(v\) and \(w\) are linearly dependent.

By dyadic partitioning on \(M(A)\), we have
\[
    M(A) = \sum _{x \in \frac{A}{A} } r_{\frac{A}{A} } (x)^{2} \leq  \log \left( \left\lvert \frac{A}{A} \right\rvert   \right) \tau ^{2} \left\lvert S \right\rvert
\]
for some \(\tau\), where \(S = \left\{ x \in \frac{A}{A}  : r_{\frac{A}{A} } (x) \asymp \tau \right\} \).

Consider the reduced system of points and lines, consisting only of the \(\left\lvert S \right\rvert \) many lines which
have \(\asymp \tau\) many points on them. Consider the set of all vector sums between points on
consecutive lines. Because all pairs of lines give disjoint sets, each with \(\asymp \tau ^{2}\) many sums, there are
\(\tau^{2}\left\lvert S \right\rvert \) many vector sums. Because this is a subset of \((A+A)^{2}\),
\[
    \tau^{2} \left\lvert S \right\rvert \leq \left\lvert A+A \right\rvert  ^{2}
,\]
so
\[
    \frac{\left\lvert A \right\rvert ^{4}}{\left\lvert AA\right\rvert } \leq M(A) \ll \log \left( \left\lvert \frac{A}{A} \right\rvert   \right)  \left\lvert A+A \right\rvert  ^{2} \implies \max \left( \left\lvert A+A \right\rvert ,\left\lvert AA \right\rvert  \right) \gg \left\lvert A \right\rvert ^{\frac{4}{3} - o(1)}
\]

\end{proof}

\textbf{This is nearly the best known result, all others are only slight improvements, ... }
\textbf{Only result purely obtaining upper bound on energy}

Another instance of this type of result is

\begin{theorem}
For convex sets \(A \subset \mathbb{R} \) which are finite and sufficiently large,
\[
    E(A) \ll \left\lvert A \right\rvert ^{\frac{32}{13} - o(1)}
,\]
\end{theorem}
which immediately leads to the result

\begin{corollary}
For convex sets \(A \subset \mathbb{R} \) which are finite and sufficiently large,
\[
    \left\lvert A+A \right\rvert \gg \left\lvert A \right\rvert ^{\frac{3}{2} + \frac{1}{26} - o(1)}
.\]
\end{corollary}

\textbf{FIX BELOW PARAGRAPH}
This is proven in \cite{shkredov-new-results-higher-energy}. I did not read this
paper because it includes a lot of unique ideas and complex notation. I plan to read it
in the future.

\textbf{REWRITE BELOW}
Many arguments involving additive energy are not as straightforward
as finding an upper bound. We often explore quantites such as \(E(A,S), E(D), E(S), E_{3} (A)\) etc.
The following are examples of those style of arguments.

I'll first prove a handful of theorems which will be useful. The first of which
I will restate from before, still leaving out proof.

\begin{theorem}
For convex sets \(A \subset \mathbb{R} \) which are finite and sufficiently large,
\[
    E_{3} (A) \ll \left\lvert A \right\rvert ^{ 3 - o(1)}
.\]
\end{theorem}

\begin{proof}
Recall that upon ordering \(a_{i} \) such that \(\delta_A(a_1) \geq \delta_A(a_2) \geq \dots \geq \delta_{A} (a_{\left\lvert A+A \right\rvert } )\),
we have that
\[
    \delta_{A} (a_{r} ) \ll \frac{\left\lvert A \right\rvert }{r^{\frac{1}{3} }}
.\]

With this,
\begin{align*}
E_3(A) & = \sum _{x \in A-A} \delta_{A} (x)^{3} \\
& \ll \left\lvert A \right\rvert ^{3} \sum_{r=1}^{\left\lvert A+A \right\rvert } \frac{1}{r}\\
& \asymp \left\lvert A \right\rvert ^{3} \int_{1}^{\left\lvert A+A \right\rvert } \frac{1}{r}  ~\mathrm{d} r \\
& = \left\lvert A \right\rvert ^{3 - o(1)}
\end{align*}
\end{proof}

\begin{theorem}
For convex sets \(A \subset \mathbb{R} \) which are finite and sufficiently large, and any finite set \(B \subset \mathbb{R} \)
\[
    E(A,B) \ll \left\lvert A \right\rvert \left\lvert B \right\rvert ^{\frac{3}{2} }
.\]
\end{theorem}

\textbf{I NEED TO REDO PROOF WITH MORE LOGIC BEHIND WHY I CHOSE B 1/2}
\begin{proof}
Denote the elements of \(A-B\) by \(s_{i} \) where \(\delta_{A,B} (s_1) \geq \cdots \geq \delta_{A,B} (s _{\left\lvert A-B \right\rvert } )\)

Let \(P = \left\{ x \in A - B : \delta_{A,B} (x) \geq \left\lvert B \right\rvert ^{\frac{1}{2} } \right\} \), and let \(P ^{*} = \left( A - B \right) \setminus P\).
\begin{align*}
    \sum _{x \in P} \delta_{A,B} (x)^{2} & =  \sum_{i=1}^{\left\lvert P \right\rvert } \delta_{A,B} (s _{r} )^{2}\\
    & \ll \left\lvert A \right\rvert ^{\frac{2}{3} } \left\lvert B \right\rvert ^{\frac{4}{3} } \sum_{i=1}^{\left\lvert P \right\rvert } \frac{1}{r^{\frac{2}{3} }} \\
    & \asymp \left\lvert A \right\rvert ^{\frac{2}{3} } \left\lvert B \right\rvert ^{\frac{4}{3} } \left\lvert P \right\rvert ^{\frac{1}{3} }\\
    & \ll \left\lvert A \right\rvert ^{\frac{2}{3} } \left\lvert B \right\rvert ^{\frac{4}{3} } \left( \frac{\left\lvert A \right\rvert \left\lvert B \right\rvert ^{2}}{\left\lvert B \right\rvert ^{\frac{1}{2} }}  \right) ^{\frac{1}{3} }\\
    & = \left\lvert A \right\rvert \left\lvert B \right\rvert ^{\frac{3}{2} },
\end{align*}
and
\[
    \sum _{x \in P^{*}} \delta_{A,B} (x)^{2} < \left\lvert B \right\rvert ^{\frac{1}{2} } \sum _{x \in P^{*}} \delta_{A,B} (x) = \left\lvert A \right\rvert \left\lvert B \right\rvert ^{\frac{3}{2} }
.\]

Therefore,
\[
    E(A,B) = \sum _{x \in P} \delta_{A,B} (x)^{2} + \sum _{x \in P ^{*}} \delta_{A,B} (x)^{2} \ll \left\lvert A \right\rvert \left\lvert B \right\rvert ^{\frac{3}{2} } 
.\]
\end{proof}

\section{ \textbf{IDK the title }}
In this section we'll prove the \textbf{... more exposition} \cite{shkredov}

Throughout this section, in the same convention as \textbf{CITED}, we'll use the
notation
\[
    A_{x} = A \cap \left( A + x \right) = \left\{ a \in A : a - x \in A \right\} 
.\]

\begin{theorem}
Let \(A \subset \mathbb{R} \) be a convex set. Then
\[
    \max \left( \left\lvert A+A \right\rvert , \left\lvert A - A \right\rvert  \right) \gg_{\epsilon}  \left\lvert A \right\rvert ^{\frac{8}{5} - \epsilon }
,\]
\[
    \left\lvert A-A \right\rvert \gg_{\epsilon}  \left\lvert A \right\rvert ^{\frac{8}{5} - \epsilon }
,\]
\[
    \left\lvert A+A \right\rvert \gg_{\epsilon}  \left\lvert A \right\rvert ^{\frac{14}{9} - \epsilon}
.\]
\end{theorem}

We require the following lemmata
\begin{lemma}
For every set \(A\) we have
\end{lemma}
%────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
%────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────

\subsection*{Acknowledgements} % Unnumbered subsection to include acknowledgements (advisor, grants, etc.) An example acknowledgement is included for reference. 
This report is based on work supported by NSF grant DMS-2051032, which we gratefully acknowledge. I would also like to express my thanks to the Mathematics department of Indiana University for hosting the program, my mentor XXX and....


%% TIP: Only entries which you actually cite with \cite somewhere else in the .tex will appear.
\bibliographystyle{amsalpha}
\bibliography{bibliography} % the file "bibliography.bib" will be needed

\section*{Appendix}

\textbf{UNIQUENESS OF REPRESENTATIONS OF DIFFERENT BASES}

\textbf{I need to define all things I use like little o and big O}.

\begin{theorem*}[Erd\H{o}s multiplication table theorem]
\[
    \left\lvert [n] \cdot [n] \right\rvert = o(n^{2})
.\]
\end{theorem*}

This is known as the ``multiplication table theorem'' because the quantity
\[
    \left\lvert [n] \cdot [n] \right\rvert 
\]
is the number of distinct numbers in an \(n \times n\) multiplication table:

\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\textwidth]{mult-table.png}
    \caption{10 \(\times \) 10 multiplication table with distinct numbers highlighted in red.}
\end{figure}

The theorem I am proving here is just an ``upper bound'' on the asymptotic behavior of
this quantity. In \cite{Ford}, Kevin Ford proved that the exact order of this quantity is
\[
    \left\lvert [n] \cdot [n] \right\rvert \asymp \frac{n^{2}}{\log \left( n^{2} \right) ^{\delta} \left( \log \log \left( n^{2} \right) \right) ^{\frac{3}{2} } } 
,\]
where \(\delta = 1 - \frac{1 + \log \log \left( 2 \right)  }{2} \) is the Erd\H{o}s-Tenenbaum-Ford constant.

A tool that will prove useful in evaluating sums is the Abel summation formula.

\begin{lemma*}
Let \(\left( a_{n}  \right) _{n = 1} ^{\infty }\) be a sequence of real numbers. Let \(A: \mathbb{R} \to \mathbb{R} \) be defined by
\[
    A(t) = \sum _{n \leq t} a_{n} 
.\]
For \(x,y \in \mathbb{R} \), with \(x < y\), and any differentiable function \(\phi : [x,y] \to \mathbb{R} \),
\[
    \sum _{x < n \leq y} a_{n} \phi(n) = A(y)\phi(y) - A(x) \phi(x) - \int_{x}^{y} A(t)\phi'(t) ~\mathrm{d} t
\]
\end{lemma*}

\textbf{FINISH THIS LATER}
\begin{proof}
\begin{align*}
    \sum _{x < n \leq y} a_{n} \phi(n) & = a_{\left\lceil x \right\rceil } \phi(\left\lceil x \right\rceil ) + \cdots  + a_{\left\lfloor y \right\rfloor } \phi(\left\lfloor y \right\rfloor )\\
    & = \left( A(x + 1) - A(x) \right) \phi(\left\lceil x \right\rceil ) + \cdots + \left( A(y) - A(y-1) \right) \phi(\left\lfloor y \right\rfloor ) \\
    & = \\
    & = A(y) \phi\left( \left\lfloor y \right\rfloor \right)  - A(x)\phi\left( \left\lceil x \right\rceil  \right) + \sum_{i= 1 }^{} \left( \phi\left( \left\lceil x \right\rceil + i \right) - \phi\left( \left\lceil x \right\rceil + 1 + i \right) \right) A(x + i)  
\end{align*}
\end{proof}

We will also need to use the Chebyshev psi function

\begin{lemma}
Let \(\psi : \mathbb{R} ^{+} \to \mathbb{N}  \) be defined by
\[
    \psi(t) = \sum _{\substack{ p ^{\alpha}\leq t \\ p \text{ prime} \\ \alpha \in \mathbb{N} }} \log \left( p \right) 
.\]
We have
\[
    \psi(t) \ll t
.\]
\end{lemma}

\begin{proof}
First we'll introduce the function \(\theta : \mathbb{R} ^{+ } \to \mathbb{N} \) defined by
\[
    \theta(t) = \sum _{\substack{ p \leq t \\ p \text{ prime}  }} \log \left( p \right)
.\]
The functions \(\theta\) and \(\psi\) are clearly related by
\[
    \psi(t) = \prod _{\substack{ p \leq N^{\frac{1}{\alpha} } \\ p \text{ prime} \\ \alpha \in \mathbb{N}  }} \log \left( p \right) = \sum _{\alpha \in \mathbb{N} } \theta\left( t ^{\frac{1}{\alpha} } \right)  = \theta(t) + \sum _{\alpha \geq 2} \theta \left( t ^{\frac{1}{\alpha} } \right) 
.\]
Note that the sum over \(\alpha\) has only finitely many terms. The sum terminates when
\[
    2 \geq  t^{\frac{1}{\alpha} } \implies  \alpha \leq \log _{2} \left( t \right) 
.\]

A trivial upper bound on \(\theta (t)\) is
\[
    \theta(t) = \sum _{\substack{ p \leq t \\ p \text{ prime}}} \log \left( p \right) \leq t \log \left( t \right)  
,\]
so
\begin{align*}
    \psi(t) & = \theta(t) + \sum _{2 \leq \alpha \leq \log _{2} \left( t \right) } \theta(t ^{\frac{1}{\alpha} }) \\
    & \leq \theta(t) + \log _{2} \left( t \right) \theta(t^{\frac{1}{2} }) \\
    & \leq \theta(t) + \frac{t^{\frac{1}{2} }\log \left( t \right) ^{2}}{\log \left( 2 \right) }
\end{align*}
or
\[
    \psi(t) \ll \max \left( \theta(t), t^{\frac{1}{2} }\log \left( t \right) ^{2} \right) 
.\]
Therefore, it suffices to show \(\theta(t) \ll t\).

We have
\[
    \theta(t) = \sum _{\substack{ p \leq t \\ p \text{ prime}  }} \log \left( p \right) = \log \left( \prod _{\substack{ p \leq t \\  p \text{ prime}  }} p \right) 
,\]
so it is sufficient to show that
\[
    \prod _{\substack{ p \leq t \\ p \text{ prime}  }} p \ll e^{t}
.\]
It is also sufficient to prove it for \(t \in \mathbb{N} \) because \(\theta(t) = \theta\left( \left\lfloor t \right\rfloor  \right) \).

For some natural number \(t\), and a prime \(p\),
\[
    t + 1 < p \leq 2t + 1 \implies p \mid \binom{2t + 1}{t} = \frac{\left( 2t + 1 \right) !}{t! (t+1)!} 
.\]
Therefore, for any \(t\),
\[
    \prod _{\substack{ t + 1 < p \leq 2t + 1\\ p \text{ prime}   }} p   \mid \binom{2t+1}{t} \implies \prod _{\substack{ t + 1 < p \leq 2t + 1\\ p \text{ prime}   }} p   \leq  \binom{2t+1}{t}
,\]
which gives us
\[
    2 \prod _{\substack{ t + 1 < p \leq 2t + 1\\ p \text{ prime}   }} p   \leq  2 \binom{2t+1}{t} \leq (1+1)^{2t + 1} \implies \prod _{\substack{ t + 1 < p \leq 2t + 1\\ p \text{ prime}   }} p  \leq 4^{t}
.\]

The rest follows by induction on \(t\). Because we are proving
a statement about order of magnitude, the base case is trivial. Now suppose that
for some \(t \in \mathbb{N} \),
\[
    \prod _{\substack{ p \leq m \\p \text{ prime}  }} p \ll e^{t}
.\]
If \(t\) is odd, the induction follows trivially. If \(t\) is even, let \(t = 2m\),
so
\[
    \prod  _{\substack{ p \leq 2m \\ p \text{ prime}  }} p\ll e^{2m}
.\]
We have
\begin{align*}
\prod _{\substack{ p \leq 2m + 1 \\ p \text{ prime}  }} p & = \prod _{\substack{ p \leq m + 1 \\ p \text{ prime}  }} p \prod _{\substack{ m + 1 < p \leq 2m + 1 \\ p \text{ prime}  }} p \\
& \ll e^{m + 1} 4^{m} \\
& \ll e^{m+1} e^{m} = e^{2m+1}. 
\end{align*}
\end{proof}

More accurately, \(\psi(t) \asymp \theta(t) \asymp t\), but this is not necessary to get the desired result,
so I am omitting it from this report.

The final lemma needed is
\begin{lemma*}
\[
    \sum _{\substack{ p \leq n \\ p\text{ prime}  }} \frac{1}{p}  = \log\log \left( n \right) + O(1)
.\]
\end{lemma*}

\begin{proof}[Proof of Lemma]
Observe that for some number \(N \in \mathbb{N} \), the prime factorization of \(N!\) is of the form
\[
    N! = \prod _{\substack{ p \leq N \\ p \text{ prime}  }} p ^{\alpha(N,p)}
,\]
where
\[
    \alpha(N,p) = \sum _{i \in \mathbb{N} } \left\lfloor \frac{N}{p ^{i}}  \right\rfloor = \sum _{i \leq \log _{p} \left( N \right) } \left\lfloor \frac{N}{p ^{i}}  \right\rfloor  =\sum _{i \leq \log _{2} \left( N \right) } \left\lfloor \frac{N}{p ^{i}}  \right\rfloor 
.\]
It follows that
\begin{align*}
    \log \left( N! \right) & = \sum _{\substack{ p \leq N \\p \text{ prime}  }} \alpha(N,p) \log \left( p \right) \\
    & = \sum _{\substack{ p \leq N \\p \text{ prime} \\ i \leq \log _{p} \left( N \right)   }} \left\lfloor \frac{N}{p ^{i}}  \right\rfloor \log \left( p \right) \\
    & = \sum _{\substack{ p \leq N \\ p \text{ prime} \\ i \leq \log _{p} \left( N \right)  }} \left( \frac{N}{p ^{i}} - \delta(p) \right) \log \left( p \right) \\
    & = N\sum _{\substack{ p \leq N \\ p \text{ prime} \\ i \leq \log _{p} \left( N \right) }} \frac{\log \left( p \right) }{p ^{i}} - \sum _{\substack{ p \leq N \\ p \text{ prime} \\ i \leq \log _{p} \left( N \right)  }} \delta(p) \log \left( p \right) .
\end{align*}
We have that
\[
    i \leq \log _{p} \left( N \right) \iff p ^{i} \leq N
,\]
so
\[
    \sum  _{\substack{ p \leq N \\ p \text{ prime} \\ i \leq \log _{p} \left( N \right)  }} \log \left( p \right) = \psi(N)
,\]
and therefore
\[
    \sum _{\substack{ p \leq N \\ p \text{ prime} \\ i \leq \log _{2} \left( N \right)  }} \frac{\log \left( p \right) }{p ^{i}} \leq  \frac{\log \left( N! \right) }{N} + \frac{\psi(N)}{N} 
.\]

We also have, via a Riemann sum,
\begin{align*}
    \log \left( N! \right) & = \sum_{i=1}^{N} \log \left( i \right) \\
    & = \int_{2}^{N} \log \left( i \right)  ~\mathrm{d} i + O(1) \\
    & = \left. \left[ i\log \left( i \right) - i \right]  \right|_{i = 1}^{N} + O(1)\\
    & = N \log \left( N \right) - N + O(1).
\end{align*}

This leads to
\[
    \sum _{\substack{ p \leq N \\ p \text{ prime} \\ i \leq \log _{2} \left( N \right) }} \frac{\log \left( p \right) }{p ^{i}} = \frac{N \log \left( N \right) - N + O(1)}{N} + \frac{O(N)}{N} = \log \left( N \right)  + O(1)
.\]

Moreover, we have that
\begin{align*}
\sum  _{\substack{ p \leq N \\ p \text{ prime} \\ 2 \leq i \leq \log _{2} \left( N \right)  }} \frac{\log \left( p \right) }{p ^{i}} & \leq \sum  _{\substack{ x,i \geq 2}} \frac{\log \left( x \right) }{x ^{i}} \\
& = \sum _{x \geq 2} \sum _{i \geq 2} \frac{\log \left( x \right) }{x^{i}} \\
& = \sum _{x \geq 2} \frac{1}{x^{2}} \left( \frac{\log \left( x \right) }{1-\frac{1}{x} }  \right)\\
& = \sum _{x \geq 2} \frac{\log \left( x \right) }{x ^{2} - x} 
\end{align*}
which, by the limit comparison test, converges if
\[
    \sum _{x \geq 2} \frac{\log \left( x \right) }{x^{2}}
\]
converges.

Apply L'Hopital's rule to see that
\[
    \forall \epsilon > 0 ~,~ \log \left( x \right) = o(x^{\epsilon})
,\]
and therefore that
\begin{align*}
\sum _{x \geq 2} \frac{\log \left( x \right)}{x ^{2}} & = \sum _{x\geq 2} \frac{o(1)}{x^{2 - \epsilon}} \\
& \ll \sum _{x \geq 2} \frac{1}{x^{2-\epsilon}} \\
& \asymp \int_{2}^{\infty} \frac{1}{x^{2-\epsilon}}  ~\mathrm{d} x 
\end{align*}
which converges for \(\epsilon < 1\). It follows that
\[
    \sum _{\substack{ p \leq N \\p \text{ prime}  }}\frac{ \log \left( p \right) }{p} = \log \left( N \right) + O(1)
.\]

Applying Abel's summation formula,
\begin{align*}
\sum  _{\substack{ p \leq N \\p \text{ prime}  }} \frac{1}{p} & = \sum _{\substack{ p \leq N \\ p \text{ prime}  }}   \frac{\log \left( p \right) }{p} \cdot \frac{1}{\log \left( p \right) } \\
& = \left( \sum _{\substack{ p \leq N \\ p \text{ prime}  }} \frac{\log \left( p \right) }{p}  \right) \cdot \frac{1}{\log \left( N \right) } + O(1) + \int_{2}^{N} \frac{ \sum _{\substack{ p \leq t \\ p \text{ prime}  }}\frac{ \log \left( p \right) }{p} }{t \log ^{2}\left( t \right) }  ~\mathrm{d} t \\
& = \frac{ \log \left( N \right) + O(1)  }{\log \left( N \right) } + O(1) + \int_{2}^{N} \frac{\log \left( t \right) + O(1)}{t \log ^{2}\left( t \right) }  ~\mathrm{d} t\\
& = O(1) + \int_{2}^{N} \frac{1}{t \log \left( t \right) }  ~\mathrm{d} t + \int_{2}^{N} \frac{O(1)}{t \log ^{2}\left( t \right) }  ~\mathrm{d} t 
\end{align*}




\end{proof}

\end{document}